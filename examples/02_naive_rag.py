# -*- coding: utf-8 -*-
"""
02. Naive RAG ì˜ˆì œ - ê¸°ë³¸ RAG íŒŒì´í”„ë¼ì¸ êµ¬í˜„

ì´ ì˜ˆì œëŠ” LangGraphë¥¼ ì‚¬ìš©í•œ ê°€ì¥ ê¸°ë³¸ì ì¸ RAG íŒŒì´í”„ë¼ì¸ì„ êµ¬í˜„í•©ë‹ˆë‹¤.
ë¬¸ì„œë¥¼ Vector Storeì— ì €ì¥í•˜ê³ , ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.

í•™ìŠµ ëª©í‘œ:
    1. RAGì˜ ê¸°ë³¸ ë™ì‘ ì›ë¦¬ ì´í•´
    2. Vector Storeì™€ Retriever ì—°ë™ ë°©ë²•
    3. ê²€ìƒ‰ â†’ ìƒì„± íŒŒì´í”„ë¼ì¸ êµ¬í˜„
    4. LangGraphì—ì„œ RAG êµ¬í˜„ íŒ¨í„´ í•™ìŠµ

ì‹¤í–‰ ë°©ë²•:
    python examples/02_naive_rag.py

í•„ìˆ˜ í™˜ê²½ ë³€ìˆ˜:
    OPENAI_API_KEY: OpenAI API í‚¤
"""

import sys
from pathlib import Path
from typing import TypedDict, List, Annotated
from operator import add

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ pathì— ì¶”ê°€
sys.path.insert(0, str(Path(__file__).parent.parent))

from langchain_core.documents import Document
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_core.prompts import ChatPromptTemplate
from langgraph.graph import StateGraph, START, END

from config.settings import get_settings
from utils.llm_factory import get_llm, get_embeddings
from utils.vector_store import VectorStoreManager


# =============================================================================
# 1. State ì •ì˜
# =============================================================================

class RAGState(TypedDict):
    """
    RAG íŒŒì´í”„ë¼ì¸ì˜ ìƒíƒœë¥¼ ì •ì˜í•©ë‹ˆë‹¤.
    
    TypedDictë¥¼ ì‚¬ìš©í•˜ì—¬ ìƒíƒœì˜ ìŠ¤í‚¤ë§ˆë¥¼ ëª…í™•í•˜ê²Œ ì •ì˜í•©ë‹ˆë‹¤.
    ê° í•„ë“œëŠ” ê·¸ë˜í”„ë¥¼ í†µí•´ ì „ë‹¬ë˜ëŠ” ë°ì´í„°ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.
    
    Attributes:
        question: ì‚¬ìš©ìì˜ ì§ˆë¬¸
        context: ê²€ìƒ‰ëœ ë¬¸ì„œ ë‚´ìš© (ë¬¸ìì—´ë¡œ ê²°í•©)
        documents: ê²€ìƒ‰ëœ Document ê°ì²´ ë¦¬ìŠ¤íŠ¸
        answer: ìƒì„±ëœ ë‹µë³€
    """
    question: str                    # ì‚¬ìš©ì ì§ˆë¬¸
    context: str                     # ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ (ë¬¸ìì—´)
    documents: List[Document]        # ê²€ìƒ‰ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
    answer: str                      # ìµœì¢… ë‹µë³€


# =============================================================================
# 2. Vector Store ì´ˆê¸°í™”
# =============================================================================

def initialize_vector_store() -> VectorStoreManager:
    """
    Vector Storeë¥¼ ì´ˆê¸°í™”í•˜ê³  ìƒ˜í”Œ ë¬¸ì„œë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
    
    Returns:
        VectorStoreManager: ì´ˆê¸°í™”ëœ Vector Store ë§¤ë‹ˆì €
    """
    print("ğŸ“š Vector Store ì´ˆê¸°í™” ì¤‘...")
    
    # ì„ë² ë”© ëª¨ë¸ ìƒì„±
    embeddings = get_embeddings()
    
    # Vector Store ë§¤ë‹ˆì € ìƒì„±
    manager = VectorStoreManager(
        embeddings=embeddings,
        collection_name="naive_rag_example",
        chunk_size=500,
        chunk_overlap=100,
    )
    
    # ìƒ˜í”Œ ë¬¸ì„œ ë¡œë“œ
    sample_file = Path(__file__).parent.parent / "data" / "sample_documents.txt"
    
    if sample_file.exists():
        manager.load_from_file(str(sample_file))
        print(f"âœ… ìƒ˜í”Œ ë¬¸ì„œ ë¡œë“œ ì™„ë£Œ: {sample_file}")
    else:
        # ìƒ˜í”Œ ë¬¸ì„œê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ ë¬¸ì„œ ì¶”ê°€
        sample_texts = [
            "LangGraphëŠ” LangChain íŒ€ì—ì„œ ê°œë°œí•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ, ìƒíƒœë¥¼ ê°€ì§„ ë‹¤ì¤‘ í–‰ìœ„ì ì• í”Œë¦¬ì¼€ì´ì…˜ì„ êµ¬ì¶•í•©ë‹ˆë‹¤.",
            "RAGëŠ” Retrieval-Augmented Generationì˜ ì•½ìë¡œ, ê²€ìƒ‰ ì¦ê°• ìƒì„±ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.",
            "Vector StoreëŠ” ì„ë² ë”© ë²¡í„°ë¥¼ ì €ì¥í•˜ê³  ìœ ì‚¬ë„ ê²€ìƒ‰ì„ ìˆ˜í–‰í•˜ëŠ” ë°ì´í„°ë² ì´ìŠ¤ì…ë‹ˆë‹¤.",
            "LangGraphì˜ í•µì‹¬ ê°œë…ì€ State, Node, Edgeì…ë‹ˆë‹¤.",
            "StateGraphëŠ” LangGraphì—ì„œ ê·¸ë˜í”„ë¥¼ êµ¬ì„±í•˜ëŠ” ë¹Œë” í´ë˜ìŠ¤ì…ë‹ˆë‹¤.",
        ]
        manager.add_texts(
            texts=sample_texts,
            metadatas=[{"source": "sample"} for _ in sample_texts]
        )
        print("âœ… ê¸°ë³¸ ìƒ˜í”Œ ë¬¸ì„œ ì¶”ê°€ ì™„ë£Œ")
    
    return manager


# ì „ì—­ Vector Store ë§¤ë‹ˆì € (í•œ ë²ˆë§Œ ì´ˆê¸°í™”)
_vector_store_manager: VectorStoreManager = None


def get_vector_store() -> VectorStoreManager:
    """Vector Store ë§¤ë‹ˆì € ì‹±ê¸€í†¤ ë°˜í™˜"""
    global _vector_store_manager
    if _vector_store_manager is None:
        _vector_store_manager = initialize_vector_store()
    return _vector_store_manager


# =============================================================================
# 3. ë…¸ë“œ í•¨ìˆ˜ ì •ì˜
# =============================================================================

def retrieve_node(state: RAGState) -> dict:
    """
    ê²€ìƒ‰ ë…¸ë“œ: ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.
    
    ì´ ë…¸ë“œëŠ” RAG íŒŒì´í”„ë¼ì¸ì˜ "R" (Retrieval) ë‹¨ê³„ì…ë‹ˆë‹¤.
    Vector Storeì—ì„œ ì§ˆë¬¸ê³¼ ìœ ì‚¬í•œ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.
    
    Args:
        state: í˜„ì¬ RAG ìƒíƒœ
    
    Returns:
        dict: ì—…ë°ì´íŠ¸ëœ ìƒíƒœ (documents, context í¬í•¨)
    
    Flow:
        1. ì‚¬ìš©ì ì§ˆë¬¸ ì¶”ì¶œ
        2. Vector Storeì—ì„œ ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰
        3. ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ì»¨í…ìŠ¤íŠ¸ ë¬¸ìì—´ë¡œ ë³€í™˜
    """
    print(f"\nğŸ” ê²€ìƒ‰ ì¤‘: '{state['question']}'")
    
    # Vector Storeì—ì„œ ë¬¸ì„œ ê²€ìƒ‰
    manager = get_vector_store()
    documents = manager.search(
        query=state["question"],
        k=3  # ìƒìœ„ 3ê°œ ë¬¸ì„œ ê²€ìƒ‰
    )
    
    print(f"   â†’ {len(documents)}ê°œ ë¬¸ì„œ ë°œê²¬")
    
    # ë¬¸ì„œ ë‚´ìš©ì„ ì»¨í…ìŠ¤íŠ¸ ë¬¸ìì—´ë¡œ ê²°í•©
    context_parts = []
    for i, doc in enumerate(documents, 1):
        context_parts.append(f"[ë¬¸ì„œ {i}]\n{doc.page_content}")
    
    context = "\n\n".join(context_parts)
    
    return {
        "documents": documents,
        "context": context,
    }


def generate_node(state: RAGState) -> dict:
    """
    ìƒì„± ë…¸ë“œ: ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
    
    ì´ ë…¸ë“œëŠ” RAG íŒŒì´í”„ë¼ì¸ì˜ "G" (Generation) ë‹¨ê³„ì…ë‹ˆë‹¤.
    LLMì„ ì‚¬ìš©í•˜ì—¬ ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
    
    Args:
        state: í˜„ì¬ RAG ìƒíƒœ (context í¬í•¨)
    
    Returns:
        dict: ì—…ë°ì´íŠ¸ëœ ìƒíƒœ (answer í¬í•¨)
    
    Flow:
        1. í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ êµ¬ì„±
        2. ì»¨í…ìŠ¤íŠ¸ì™€ ì§ˆë¬¸ì„ í”„ë¡¬í”„íŠ¸ì— í¬í•¨
        3. LLM í˜¸ì¶œí•˜ì—¬ ë‹µë³€ ìƒì„±
    """
    print("\nğŸ’­ ë‹µë³€ ìƒì„± ì¤‘...")
    
    # LLM ìƒì„±
    llm = get_llm()
    
    # RAG í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
    # ì»¨í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•˜ë„ë¡ ì§€ì‹œí•©ë‹ˆë‹¤
    prompt = ChatPromptTemplate.from_messages([
        ("system", """ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
ì•„ë˜ ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.

ì¤‘ìš”:
- ì»¨í…ìŠ¤íŠ¸ì— ìˆëŠ” ì •ë³´ë§Œ ì‚¬ìš©í•˜ì„¸ìš”
- ì»¨í…ìŠ¤íŠ¸ì— ë‹µì´ ì—†ìœ¼ë©´ "ì œê³µëœ ì •ë³´ì—ì„œ ë‹µì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë§í•˜ì„¸ìš”
- ë‹µë³€ì€ ëª…í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”

ì»¨í…ìŠ¤íŠ¸:
{context}"""),
        ("human", "{question}"),
    ])
    
    # í”„ë¡¬í”„íŠ¸ êµ¬ì„± ë° LLM í˜¸ì¶œ
    chain = prompt | llm
    
    response = chain.invoke({
        "context": state["context"],
        "question": state["question"],
    })
    
    print("   â†’ ë‹µë³€ ìƒì„± ì™„ë£Œ")
    
    return {"answer": response.content}


# =============================================================================
# 4. RAG ê·¸ë˜í”„ ìƒì„±
# =============================================================================

def create_rag_graph():
    """
    Naive RAG ê·¸ë˜í”„ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    
    ê·¸ë˜í”„ êµ¬ì¡° (ë‹¨ìˆœ ì„ í˜• íŒŒì´í”„ë¼ì¸):
        START â†’ retrieve â†’ generate â†’ END
    
    Returns:
        CompiledGraph: ì»´íŒŒì¼ëœ RAG ê·¸ë˜í”„
    
    Note:
        Naive RAGëŠ” ê°€ì¥ ë‹¨ìˆœí•œ í˜•íƒœì˜ RAGì…ë‹ˆë‹¤:
        1. ì§ˆë¬¸ì„ ë°›ìŒ
        2. ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
        3. ê²€ìƒ‰ëœ ë¬¸ì„œë¡œ ë‹µë³€ ìƒì„±
        
        ë‹¨ì :
        - ê²€ìƒ‰ ê²°ê³¼ì˜ í’ˆì§ˆì„ ê²€ì¦í•˜ì§€ ì•ŠìŒ
        - ë‹µë³€ì˜ ì •í™•ì„±ì„ í™•ì¸í•˜ì§€ ì•ŠìŒ
        - ë°˜ë³µì ì¸ ê²€ìƒ‰ì´ ë¶ˆê°€ëŠ¥
    """
    # StateGraph ìƒì„± (RAGState ì‚¬ìš©)
    graph = StateGraph(RAGState)
    
    # ----- ë…¸ë“œ ì¶”ê°€ -----
    graph.add_node("retrieve", retrieve_node)
    graph.add_node("generate", generate_node)
    
    # ----- ì—£ì§€ ì¶”ê°€ -----
    # ë‹¨ìˆœ ì„ í˜• íŒŒì´í”„ë¼ì¸: START â†’ retrieve â†’ generate â†’ END
    graph.add_edge(START, "retrieve")
    graph.add_edge("retrieve", "generate")
    graph.add_edge("generate", END)
    
    # ê·¸ë˜í”„ ì»´íŒŒì¼
    compiled_graph = graph.compile()
    
    print("âœ… Naive RAG ê·¸ë˜í”„ ì»´íŒŒì¼ ì™„ë£Œ!")
    return compiled_graph


# =============================================================================
# 5. RAG ì‹¤í–‰
# =============================================================================

def run_rag(question: str) -> str:
    """
    RAG íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.
    
    Args:
        question: ì‚¬ìš©ì ì§ˆë¬¸
    
    Returns:
        str: ìƒì„±ëœ ë‹µë³€
    """
    # ê·¸ë˜í”„ ìƒì„±
    graph = create_rag_graph()
    
    # ì´ˆê¸° ìƒíƒœ ì„¤ì •
    initial_state = {
        "question": question,
        "context": "",
        "documents": [],
        "answer": "",
    }
    
    print(f"\n{'='*60}")
    print(f"ğŸ™‹ ì§ˆë¬¸: {question}")
    print('='*60)
    
    # ê·¸ë˜í”„ ì‹¤í–‰
    result = graph.invoke(initial_state)
    
    # ê²°ê³¼ ì¶œë ¥
    print(f"\nğŸ“š ê²€ìƒ‰ëœ ë¬¸ì„œ ìˆ˜: {len(result['documents'])}")
    print(f"\nğŸ¤– ë‹µë³€:\n{result['answer']}")
    print('='*60)
    
    return result["answer"]


def run_rag_with_stream(question: str):
    """
    ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œë¡œ RAGë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.
    
    ê° ë…¸ë“œì˜ ì‹¤í–‰ ê³¼ì •ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    
    Args:
        question: ì‚¬ìš©ì ì§ˆë¬¸
    """
    graph = create_rag_graph()
    
    initial_state = {
        "question": question,
        "context": "",
        "documents": [],
        "answer": "",
    }
    
    print(f"\n{'='*60}")
    print(f"ğŸ™‹ ì§ˆë¬¸: {question}")
    print('='*60)
    
    # stream()ìœ¼ë¡œ ê° ë‹¨ê³„ í™•ì¸
    for event in graph.stream(initial_state, stream_mode="updates"):
        # eventëŠ” {ë…¸ë“œëª…: ìƒíƒœ ì—…ë°ì´íŠ¸} í˜•íƒœ
        for node_name, updates in event.items():
            print(f"\nğŸ“ ë…¸ë“œ: {node_name}")
            
            if "documents" in updates:
                print(f"   â†’ ê²€ìƒ‰ëœ ë¬¸ì„œ: {len(updates['documents'])}ê°œ")
            
            if "context" in updates:
                preview = updates["context"][:100] + "..." if len(updates.get("context", "")) > 100 else updates.get("context", "")
                print(f"   â†’ ì»¨í…ìŠ¤íŠ¸ ë¯¸ë¦¬ë³´ê¸°: {preview}")
            
            if "answer" in updates:
                print(f"   â†’ ë‹µë³€: {updates['answer'][:200]}...")


# =============================================================================
# 6. ì‹œê°í™” (ì„ íƒ)
# =============================================================================

def visualize_graph():
    """
    RAG ê·¸ë˜í”„ êµ¬ì¡°ë¥¼ ì‹œê°í™”í•©ë‹ˆë‹¤.
    
    Mermaid ë‹¤ì´ì–´ê·¸ë¨ í˜•ì‹ìœ¼ë¡œ ê·¸ë˜í”„ êµ¬ì¡°ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.
    """
    print("\nğŸ“Š RAG ê·¸ë˜í”„ êµ¬ì¡° (Mermaid)")
    print("```mermaid")
    print("graph TD")
    print("    START((START)) --> retrieve[ê²€ìƒ‰ ë…¸ë“œ]")
    print("    retrieve --> generate[ìƒì„± ë…¸ë“œ]")
    print("    generate --> END((END))")
    print("")
    print("    subgraph State")
    print("        Q[question]")
    print("        C[context]")
    print("        D[documents]")
    print("        A[answer]")
    print("    end")
    print("```")


# =============================================================================
# ë©”ì¸ ì‹¤í–‰
# =============================================================================

if __name__ == "__main__":
    print("\n" + "="*60)
    print("Naive RAG ì˜ˆì œ - ê¸°ë³¸ RAG íŒŒì´í”„ë¼ì¸")
    print("="*60)
    
    # ì„¤ì • í™•ì¸
    settings = get_settings()
    if not settings.validate_openai_key():
        print("\nâš ï¸ OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("ğŸ“ .env íŒŒì¼ì— OPENAI_API_KEYë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
        sys.exit(1)
    
    # ê·¸ë˜í”„ ì‹œê°í™”
    visualize_graph()
    
    # í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ ì‹¤í–‰
    test_queries = [
        "LangGraphë€ ë¬´ì—‡ì¸ê°€ìš”?",
        "RAGì˜ ê¸°ë³¸ êµ¬ì„± ìš”ì†ŒëŠ” ë¬´ì—‡ì¸ê°€ìš”?",
        "StateGraphëŠ” ì–´ë–¤ ì—­í• ì„ í•˜ë‚˜ìš”?",
    ]
    
    for query in test_queries:
        try:
            run_rag(query)
        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
            import traceback
            traceback.print_exc()
        
        print("\n")
