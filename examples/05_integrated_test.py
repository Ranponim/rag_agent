# -*- coding: utf-8 -*-
"""
05. Integrated Test - ëª¨ë“  ê¸°ë²•ì„ í†µí•©í•œ ìµœì¢… ì˜ˆì œ

ì´ ì˜ˆì œëŠ” ì§€ê¸ˆê¹Œì§€ í•™ìŠµí•œ ëª¨ë“  LangGraph ê¸°ë²•ì„ í†µí•©í•˜ì—¬
ì‹¤ì „ ìˆ˜ì¤€ì˜ RAG Agent ì‹œìŠ¤í…œì„ êµ¬í˜„í•©ë‹ˆë‹¤.

í†µí•©ëœ ê¸°ë²•:
    1. Multi-Agent (Supervisor íŒ¨í„´)
    2. Memory (ëŒ€í™” ê¸°ë¡ ìœ ì§€)
    3. Adaptive RAG (ì¿¼ë¦¬ ë³µì¡ë„ ë¶„ë¥˜)
    4. Tool Calling (ì™¸ë¶€ ë„êµ¬ í™œìš©)
    5. Document Grading (ë¬¸ì„œ ê´€ë ¨ì„± í‰ê°€)
    6. Query Transform (ì¿¼ë¦¬ ë³€í™˜)

ì‹¤í–‰: python examples/05_integrated_test.py
"""

import sys
from pathlib import Path
from typing import TypedDict, List, Literal, Annotated

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ ê²½ë¡œì— ì¶”ê°€í•˜ì—¬ ë‚´ë¶€ ëª¨ë“ˆ(config, utils)ì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ìˆê²Œ í•¨
sys.path.insert(0, str(Path(__file__).parent.parent))

# LangChain: ë©”ì‹œì§€ êµ¬ì¡°, ë„êµ¬ ì •ì˜ ë° RAG ê´€ë ¨
from langchain_core.documents import Document  # í‘œì¤€ ë¬¸ì„œ ê°ì²´
from langchain_core.messages import HumanMessage, SystemMessage, AIMessage, BaseMessage  # ë‹¤ì–‘í•œ ë©”ì‹œì§€ íƒ€ì…
from langchain_core.prompts import ChatPromptTemplate  # í”„ë¡¬í”„íŠ¸ ì„¤ê³„ë„
from langchain_core.tools import tool  # ë„êµ¬ ì •ì˜ ë°ì½”ë ˆì´í„°

# LangGraph: ì›Œí¬í”Œë¡œìš° ì œì–´, ìƒíƒœ ê´€ë¦¬ ë° ì²´í¬í¬ì¸íŠ¸
from langgraph.graph import StateGraph, START, END  # ê·¸ë˜í”„ ë¹Œë” ë° ì£¼ìš” ì œì–´ í¬ì¸íŠ¸
from langgraph.graph.message import add_messages  # ë©”ì‹œì§€ ìë™ ë³‘í•© ë¦¬ë“€ì„œ
from langgraph.prebuilt import ToolNode  # í‘œì¤€ ë„êµ¬ ì‹¤í–‰ ë…¸ë“œ
from langgraph.checkpoint.memory import MemorySaver  # ëŒ€í™” ê¸°ë¡ ì˜ì†ì„± ê´€ë¦¬ë¥¼ ìœ„í•œ ì²´í¬í¬ì¸í„°

# í”„ë¡œì íŠ¸ ìœ í‹¸ë¦¬í‹°
from config.settings import get_settings  # ì„¤ì • ë° í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
from utils.llm_factory import get_llm, get_embeddings  # LLM/ì„ë² ë”© íŒ©í† ë¦¬
from utils.vector_store import VectorStoreManager  # ë²¡í„° DB ê²€ìƒ‰ ë§¤ë‹ˆì €


# =============================================================================
# 1. í†µí•© State ì •ì˜
# =============================================================================

class IntegratedAgentState(TypedDict):
    """í†µí•© Agent ì‹œìŠ¤í…œì˜ ìƒíƒœ"""
    # ë©”ì‹œì§€ íˆìŠ¤í† ë¦¬ (Memory)
    messages: Annotated[List[BaseMessage], add_messages]
    
    # ì¿¼ë¦¬ ë¶„ì„
    current_query: str
    query_type: str                      # "chat" | "search" | "tool"
    query_complexity: str                # "simple" | "moderate" | "complex"
    
    # RAG ê´€ë ¨
    transformed_query: str               # ë³€í™˜ëœ ì¿¼ë¦¬
    documents: List[Document]            # ê²€ìƒ‰ëœ ë¬¸ì„œ
    graded_documents: List[Document]     # í‰ê°€ëœ ë¬¸ì„œ
    context: str
    
    # ì‹¤í–‰ ì¶”ì 
    current_agent: str
    steps_taken: List[str]


# =============================================================================
# 2. Vector Store ì´ˆê¸°í™”
# =============================================================================

_integrated_vs: VectorStoreManager = None

def get_integrated_vs() -> VectorStoreManager:
    global _integrated_vs
    if _integrated_vs is None:
        print("ğŸ“š í†µí•© ì‹œìŠ¤í…œ Vector Store ì´ˆê¸°í™”...")
        _integrated_vs = VectorStoreManager(
            embeddings=get_embeddings(),
            collection_name="integrated_system",
        )
        samples = [
            "LangGraphëŠ” ìƒíƒœ ê¸°ë°˜ ì—ì´ì „íŠ¸ë¥¼ êµ¬ì¶•í•˜ê¸° ìœ„í•œ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤. StateGraphë¡œ ë…¸ë“œì™€ ì—£ì§€ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.",
            "RAG(Retrieval-Augmented Generation)ëŠ” ê²€ìƒ‰ ì¦ê°• ìƒì„±ìœ¼ë¡œ, LLMì— ì™¸ë¶€ ì§€ì‹ì„ ì œê³µí•©ë‹ˆë‹¤.",
            "Multi-Agent ì‹œìŠ¤í…œì€ ì—¬ëŸ¬ ì „ë¬¸ Agentê°€ í˜‘ë ¥í•˜ì—¬ ë³µì¡í•œ ì‘ì—…ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.",
            "MemorySaverëŠ” LangGraphì—ì„œ ëŒ€í™” ê¸°ë¡ì„ ì €ì¥í•˜ê³  ë³µì›í•˜ëŠ” ì²´í¬í¬ì¸í„°ì…ë‹ˆë‹¤.",
            "Adaptive RAGëŠ” ì¿¼ë¦¬ ë³µì¡ë„ì— ë”°ë¼ ë‹¤ë¥¸ RAG ì „ëµì„ ì„ íƒí•©ë‹ˆë‹¤.",
            "Tool Callingì€ LLMì´ ì™¸ë¶€ ë„êµ¬ë¥¼ í˜¸ì¶œí•˜ì—¬ ì‹¤ì‹œê°„ ì •ë³´ë¥¼ ì–»ëŠ” ê¸°ë²•ì…ë‹ˆë‹¤.",
            "Document Gradingì€ ê²€ìƒ‰ëœ ë¬¸ì„œì˜ ê´€ë ¨ì„±ì„ í‰ê°€í•˜ì—¬ í’ˆì§ˆì„ ë³´ì¥í•©ë‹ˆë‹¤.",
            "Query Transformì€ ì›ë³¸ ì¿¼ë¦¬ë¥¼ ë³€í™˜í•˜ì—¬ ê²€ìƒ‰ íš¨ìœ¨ì„ ë†’ì…ë‹ˆë‹¤. HyDE, Multi-Query ë“±ì´ ìˆìŠµë‹ˆë‹¤.",
        ]
        _integrated_vs.add_texts(texts=samples)
        print(f"âœ… {len(samples)}ê°œ ë¬¸ì„œ ì¶”ê°€")
    return _integrated_vs


# =============================================================================
# 3. ë„êµ¬ ì •ì˜
# =============================================================================

@tool
def get_current_time() -> str:
    """í˜„ì¬ ì‹œê°„ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    from datetime import datetime
    return f"í˜„ì¬ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"


@tool
def calculate(expression: str) -> str:
    """ìˆ˜í•™ ê³„ì‚°ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
    try:
        return f"{expression} = {eval(expression)}"
    except:
        return "ê³„ì‚° ì˜¤ë¥˜"


@tool
def search_web(query: str) -> str:
    """ì›¹ì—ì„œ ì •ë³´ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤. (ë°ëª¨ìš©)"""
    return f"'{query}' ê²€ìƒ‰ ê²°ê³¼: ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤."


tools = [get_current_time, calculate, search_web]


# =============================================================================
# 4. ë…¸ë“œ í•¨ìˆ˜ë“¤
# =============================================================================

def router_node(state: IntegratedAgentState) -> dict:
    """
    Router: ì¿¼ë¦¬ë¥¼ ë¶„ì„í•˜ì—¬ ì ì ˆí•œ ì²˜ë¦¬ ê²½ë¡œ ê²°ì •
    
    - chat: ì¼ë°˜ ëŒ€í™” â†’ ì§ì ‘ ì‘ë‹µ
    - search: ì •ë³´ ê²€ìƒ‰ í•„ìš” â†’ RAG íŒŒì´í”„ë¼ì¸
    - tool: ë„êµ¬ ì‚¬ìš© í•„ìš” â†’ Tool Agent
    """
    print("\nğŸ”€ [Router] ì¿¼ë¦¬ ë¶„ì„ ì¤‘...")
    
    # ë§ˆì§€ë§‰ ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ì¶œ
    last_message = state["messages"][-1]
    query = last_message.content if hasattr(last_message, "content") else str(last_message)
    
    llm = get_llm()
    
    prompt = ChatPromptTemplate.from_messages([
        ("system", """ì¿¼ë¦¬ë¥¼ ë¶„ì„í•˜ì—¬ ì²˜ë¦¬ ë°©ì‹ì„ ê²°ì •í•˜ì„¸ìš”.

- chat: ì¸ì‚¬, ì¡ë‹´, ì´ì „ ëŒ€í™” ì°¸ì¡° ë“±
- search: ì •ë³´ ê²€ìƒ‰ì´ í•„ìš”í•œ ì§ˆë¬¸ (LangGraph, RAG ë“±)
- tool: ê³„ì‚°, í˜„ì¬ ì‹œê°„, ì›¹ ê²€ìƒ‰ ë“± ë„êµ¬ê°€ í•„ìš”í•œ ê²½ìš°

"chat", "search", "tool" ì¤‘ í•˜ë‚˜ë§Œ ë‹µí•˜ì„¸ìš”."""),
        ("human", "ì¿¼ë¦¬: {query}"),
    ])
    
    response = (prompt | llm).invoke({"query": query})
    content = response.content.lower().strip()
    
    if "tool" in content:
        query_type = "tool"
    elif "search" in content:
        query_type = "search"
    else:
        query_type = "chat"
    
    print(f"   â†’ ì¿¼ë¦¬ ìœ í˜•: {query_type}")
    
    return {
        "current_query": query,
        "query_type": query_type,
        "steps_taken": state.get("steps_taken", []) + ["router"]
    }


def chat_node(state: IntegratedAgentState) -> dict:
    """ì¼ë°˜ ëŒ€í™” ì²˜ë¦¬"""
    print("\nğŸ’¬ [Chat] ëŒ€í™” ì‘ë‹µ ìƒì„±...")
    
    llm = get_llm()
    
    # ì´ì „ ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ í¬í•¨
    messages = [
        SystemMessage(content="ë‹¹ì‹ ì€ ì¹œì ˆí•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì´ì „ ëŒ€í™”ë¥¼ ì°¸ê³ í•˜ì—¬ ìì—°ìŠ¤ëŸ½ê²Œ ëŒ€í™”í•˜ì„¸ìš”.")
    ] + state["messages"]
    
    response = llm.invoke(messages)
    
    return {
        "messages": [response],
        "steps_taken": state.get("steps_taken", []) + ["chat"]
    }


def query_transform_node(state: IntegratedAgentState) -> dict:
    """ì¿¼ë¦¬ ë³€í™˜ (HyDE ìŠ¤íƒ€ì¼)"""
    print("\nğŸ”„ [Query Transform] ì¿¼ë¦¬ ë³€í™˜ ì¤‘...")
    
    llm = get_llm()
    
    prompt = ChatPromptTemplate.from_messages([
        ("system", "ì§ˆë¬¸ì— ëŒ€í•œ ì´ìƒì ì¸ ë‹µë³€ì„ ì‘ì„±í•˜ì„¸ìš”. ì´ ë‹µë³€ì€ ê²€ìƒ‰ì— ì‚¬ìš©ë©ë‹ˆë‹¤."),
        ("human", "{query}"),
    ])
    
    response = (prompt | llm).invoke({"query": state["current_query"]})
    transformed = response.content
    
    print(f"   â†’ ë³€í™˜ëœ ì¿¼ë¦¬: {transformed[:80]}...")
    
    return {
        "transformed_query": transformed,
        "steps_taken": state.get("steps_taken", []) + ["query_transform"]
    }


def retrieve_node(state: IntegratedAgentState) -> dict:
    """ë¬¸ì„œ ê²€ìƒ‰"""
    print("\nğŸ” [Retrieve] ë¬¸ì„œ ê²€ìƒ‰ ì¤‘...")
    
    vs = get_integrated_vs()
    
    # ë³€í™˜ëœ ì¿¼ë¦¬ ë˜ëŠ” ì›ë³¸ ì¿¼ë¦¬ ì‚¬ìš©
    search_query = state.get("transformed_query") or state["current_query"]
    docs = vs.search(query=search_query, k=5)
    
    print(f"   â†’ {len(docs)}ê°œ ë¬¸ì„œ ê²€ìƒ‰ë¨")
    
    return {
        "documents": docs,
        "steps_taken": state.get("steps_taken", []) + ["retrieve"]
    }


def grade_documents_node(state: IntegratedAgentState) -> dict:
    """ë¬¸ì„œ ê´€ë ¨ì„± í‰ê°€"""
    print("\nğŸ“Š [Grade] ë¬¸ì„œ ê´€ë ¨ì„± í‰ê°€...")
    
    llm = get_llm()
    
    prompt = ChatPromptTemplate.from_messages([
        ("system", "ë¬¸ì„œê°€ ì§ˆë¬¸ê³¼ ê´€ë ¨ìˆìœ¼ë©´ 'yes', ì—†ìœ¼ë©´ 'no'ë§Œ ë‹µí•˜ì„¸ìš”."),
        ("human", "ì§ˆë¬¸: {query}\n\në¬¸ì„œ: {document}"),
    ])
    
    graded = []
    for i, doc in enumerate(state["documents"]):
        response = (prompt | llm).invoke({
            "query": state["current_query"],
            "document": doc.page_content[:500]
        })
        
        if "yes" in response.content.lower():
            graded.append(doc)
            print(f"   [{i+1}] âœ… ê´€ë ¨ ìˆìŒ")
        else:
            print(f"   [{i+1}] âŒ ê´€ë ¨ ì—†ìŒ")
    
    context = "\n\n".join([doc.page_content for doc in graded[:3]])
    
    print(f"   â†’ ê´€ë ¨ ë¬¸ì„œ: {len(graded)}ê°œ")
    
    return {
        "graded_documents": graded,
        "context": context,
        "steps_taken": state.get("steps_taken", []) + ["grade"]
    }


def generate_node(state: IntegratedAgentState) -> dict:
    """RAG ë‹µë³€ ìƒì„±"""
    print("\nğŸ’­ [Generate] ë‹µë³€ ìƒì„± ì¤‘...")
    
    llm = get_llm()
    
    prompt = ChatPromptTemplate.from_messages([
        ("system", """ì»¨í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.

ì»¨í…ìŠ¤íŠ¸:
{context}"""),
        ("human", "{query}"),
    ])
    
    response = (prompt | llm).invoke({
        "context": state.get("context", "ì •ë³´ ì—†ìŒ"),
        "query": state["current_query"]
    })
    
    return {
        "messages": [AIMessage(content=response.content)],
        "steps_taken": state.get("steps_taken", []) + ["generate"]
    }


def tool_agent_node(state: IntegratedAgentState) -> dict:
    """ë„êµ¬ ì‚¬ìš© Agent"""
    print("\nğŸ”§ [Tool Agent] ë„êµ¬ í˜¸ì¶œ ì¤‘...")
    
    llm = get_llm()
    llm_with_tools = llm.bind_tools(tools)
    
    messages = [
        SystemMessage(content="í•„ìš”í•œ ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”.")
    ] + state["messages"]
    
    response = llm_with_tools.invoke(messages)
    
    return {
        "messages": [response],
        "steps_taken": state.get("steps_taken", []) + ["tool_agent"]
    }


def tool_executor_node(state: IntegratedAgentState) -> dict:
    """ë„êµ¬ ì‹¤í–‰"""
    print("\nâš™ï¸ [Tool Executor] ë„êµ¬ ì‹¤í–‰...")
    
    tool_node = ToolNode(tools)
    result = tool_node.invoke(state)
    
    return {
        "messages": result.get("messages", []),
        "steps_taken": state.get("steps_taken", []) + ["tool_executor"]
    }


def should_use_tools(state: IntegratedAgentState) -> Literal["tools", "end"]:
    """ë„êµ¬ í˜¸ì¶œ ì—¬ë¶€ í™•ì¸"""
    last_message = state["messages"][-1]
    
    if hasattr(last_message, "tool_calls") and last_message.tool_calls:
        return "tools"
    return "end"


# =============================================================================
# 5. ë¼ìš°í„° í•¨ìˆ˜
# =============================================================================

def route_by_query_type(state: IntegratedAgentState) -> Literal["chat", "rag", "tool"]:
    """ì¿¼ë¦¬ ìœ í˜•ì— ë”°ë¼ ë¼ìš°íŒ…"""
    query_type = state.get("query_type", "chat")
    
    if query_type == "search":
        return "rag"
    elif query_type == "tool":
        return "tool"
    return "chat"


# =============================================================================
# 6. ê·¸ë˜í”„ ìƒì„±
# =============================================================================

def create_integrated_agent():
    """
    í†µí•© Agent ê·¸ë˜í”„ ìƒì„±
    
    êµ¬ì¡°:
        START â†’ router â†’ (chat | rag | tool)
        
        chat â†’ END
        
        rag: query_transform â†’ retrieve â†’ grade â†’ generate â†’ END
        
        tool: tool_agent â†’ (tools â†’ tool_agent) | END
    """
    graph = StateGraph(IntegratedAgentState)
    
    # ë…¸ë“œ ì¶”ê°€
    graph.add_node("router", router_node)
    graph.add_node("chat", chat_node)
    graph.add_node("query_transform", query_transform_node)
    graph.add_node("retrieve", retrieve_node)
    graph.add_node("grade", grade_documents_node)
    graph.add_node("generate", generate_node)
    graph.add_node("tool_agent", tool_agent_node)
    graph.add_node("tools", tool_executor_node)
    
    # ì‹œì‘ â†’ ë¼ìš°í„°
    graph.add_edge(START, "router")
    
    # ë¼ìš°í„° ë¶„ê¸°
    graph.add_conditional_edges(
        "router",
        route_by_query_type,
        {
            "chat": "chat",
            "rag": "query_transform",
            "tool": "tool_agent"
        }
    )
    
    # Chat ê²½ë¡œ
    graph.add_edge("chat", END)
    
    # RAG ê²½ë¡œ
    graph.add_edge("query_transform", "retrieve")
    graph.add_edge("retrieve", "grade")
    graph.add_edge("grade", "generate")
    graph.add_edge("generate", END)
    
    # Tool ê²½ë¡œ
    graph.add_conditional_edges(
        "tool_agent",
        should_use_tools,
        {
            "tools": "tools",
            "end": END
        }
    )
    graph.add_edge("tools", "tool_agent")
    
    # ë©”ëª¨ë¦¬ í™œì„±í™”
    memory = MemorySaver()
    compiled = graph.compile(checkpointer=memory)
    
    print("âœ… í†µí•© Agent ì‹œìŠ¤í…œ ì»´íŒŒì¼ ì™„ë£Œ!")
    return compiled


# =============================================================================
# 7. ì‹¤í–‰ ì¸í„°í˜ì´ìŠ¤
# =============================================================================

def chat_with_agent(graph, thread_id: str, message: str) -> str:
    """Agentì™€ ëŒ€í™”"""
    config = {"configurable": {"thread_id": thread_id}}
    
    print(f"\n{'='*60}")
    print(f"ğŸ™‹ [{thread_id}] ì‚¬ìš©ì: {message}")
    print('='*60)
    
    result = graph.invoke(
        {
            "messages": [HumanMessage(content=message)],
            "current_query": "",
            "query_type": "",
            "query_complexity": "",
            "transformed_query": "",
            "documents": [],
            "graded_documents": [],
            "context": "",
            "current_agent": "",
            "steps_taken": []
        },
        config=config
    )
    
    # ì‹¤í–‰ ê²½ë¡œ ì¶œë ¥
    steps = result.get("steps_taken", [])
    print(f"\nğŸ“ ì‹¤í–‰ ê²½ë¡œ: {' â†’ '.join(steps)}")
    
    # ìµœì¢… ì‘ë‹µ
    final_message = result["messages"][-1]
    response = final_message.content if hasattr(final_message, "content") else str(final_message)
    
    print(f"\nğŸ¤– [{thread_id}] Agent: {response}")
    print('='*60)
    
    return response


# =============================================================================
# ë©”ì¸ ì‹¤í–‰
# =============================================================================

if __name__ == "__main__":
    from utils.llm_factory import log_llm_error
    
    print("\n" + "="*60)
    print("ğŸš€ í†µí•© í…ŒìŠ¤íŠ¸ - ëª¨ë“  ê¸°ë²• ê²°í•©")
    print("="*60)
    
    try:
        graph = create_integrated_agent()
        
        # í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤
        print("\nğŸ“Œ ì‹œë‚˜ë¦¬ì˜¤: ë‹¤ì–‘í•œ ìœ í˜•ì˜ ì§ˆë¬¸")
        
        # 1. ì¼ë°˜ ëŒ€í™”
        chat_with_agent(graph, "test-session", "ì•ˆë…•í•˜ì„¸ìš”!")
        
        # 2. ì •ë³´ ê²€ìƒ‰ (RAG)
        chat_with_agent(graph, "test-session", "LangGraphê°€ ë­ì•¼?")
        
        # 3. ë„êµ¬ ì‚¬ìš©
        chat_with_agent(graph, "test-session", "ì§€ê¸ˆ ëª‡ ì‹œì•¼?")
        
        # 4. ê³„ì‚°
        chat_with_agent(graph, "test-session", "123 * 456 ê³„ì‚°í•´ì¤˜")
        
        # 5. ì´ì „ ëŒ€í™” ì°¸ì¡° (Memory)
        chat_with_agent(graph, "test-session", "ì•„ê¹Œ LangGraphì— ëŒ€í•´ ë­ë¼ê³  í–ˆì§€?")
        
        print("\n" + "="*60)
        print("âœ… í†µí•© í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        print("="*60)
        
    except Exception as e:
        log_llm_error(e)
        print(f"âŒ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
