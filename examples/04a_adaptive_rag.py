# -*- coding: utf-8 -*-
"""
04a. Adaptive RAG - ì¿¼ë¦¬ ë³µì¡ë„ì— ë”°ë¥¸ ì ì‘í˜• RAG

ì´ ì˜ˆì œëŠ” ì¿¼ë¦¬ì˜ ë³µì¡ë„ë¥¼ ë¶„ì„í•˜ì—¬ ì ì ˆí•œ RAG ì „ëµì„ 
ë™ì ìœ¼ë¡œ ì„ íƒí•˜ëŠ” Adaptive RAGë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤.

í•™ìŠµ ëª©í‘œ:
    1. ì¿¼ë¦¬ ë³µì¡ë„ ë¶„ë¥˜ (ë‹¨ìˆœ/ì¤‘ê°„/ë³µì¡)
    2. ì „ëµë³„ ë‹¤ë¥¸ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    3. ë™ì  ë¼ìš°íŒ…
    4. ë¹„ìš©-í’ˆì§ˆ íŠ¸ë ˆì´ë“œì˜¤í”„

ì‹¤í–‰: python examples/04a_adaptive_rag.py
"""

import sys
from pathlib import Path
from typing import TypedDict, List, Literal

sys.path.insert(0, str(Path(__file__).parent.parent))

from langchain_core.documents import Document
from langchain_core.prompts import ChatPromptTemplate
from langgraph.graph import StateGraph, START, END

from config.settings import get_settings
from utils.llm_factory import get_llm, get_embeddings
from utils.vector_store import VectorStoreManager


# =============================================================================
# 1. State ì •ì˜
# =============================================================================

class AdaptiveRAGState(TypedDict):
    """Adaptive RAG ìƒíƒœ"""
    question: str
    query_complexity: str            # "simple" | "moderate" | "complex"
    strategy_used: str               # ì‚¬ìš©ëœ ì „ëµ
    documents: List[Document]
    context: str
    answer: str


# =============================================================================
# 2. Vector Store ì´ˆê¸°í™”
# =============================================================================

_adaptive_vs: VectorStoreManager = None

def get_adaptive_vs() -> VectorStoreManager:
    global _adaptive_vs
    if _adaptive_vs is None:
        print("ğŸ“š Adaptive RAG Vector Store ì´ˆê¸°í™”...")
        _adaptive_vs = VectorStoreManager(
            embeddings=get_embeddings(),
            collection_name="adaptive_rag",
        )
        samples = [
            "LangGraphëŠ” ìƒíƒœ ê¸°ë°˜ ì—ì´ì „íŠ¸ë¥¼ êµ¬ì¶•í•˜ê¸° ìœ„í•œ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.",
            "RAGëŠ” Retrieval-Augmented Generationì˜ ì•½ìë¡œ, ê²€ìƒ‰ ì¦ê°• ìƒì„±ì…ë‹ˆë‹¤.",
            "Adaptive RAGëŠ” ì¿¼ë¦¬ ë³µì¡ë„ì— ë”°ë¼ ë‹¤ë¥¸ ì „ëµì„ ì‚¬ìš©í•©ë‹ˆë‹¤.",
            "Self-RAGëŠ” LLMì´ ê²€ìƒ‰ í•„ìš”ì„±ê³¼ ë‹µë³€ í’ˆì§ˆì„ ìŠ¤ìŠ¤ë¡œ í‰ê°€í•©ë‹ˆë‹¤.",
            "Vector StoreëŠ” ì„ë² ë”© ë²¡í„°ë¥¼ ì €ì¥í•˜ê³  ìœ ì‚¬ë„ ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.",
            "ì„ë² ë”©ì€ í…ìŠ¤íŠ¸ë¥¼ ê³ ì°¨ì› ë²¡í„°ë¡œ ë³€í™˜í•˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤.",
        ]
        _adaptive_vs.add_texts(texts=samples)
        print(f"âœ… {len(samples)}ê°œ ë¬¸ì„œ ì¶”ê°€")
    return _adaptive_vs


# =============================================================================
# 3. ì¿¼ë¦¬ ë³µì¡ë„ ë¶„ë¥˜
# =============================================================================

def classify_query_node(state: AdaptiveRAGState) -> dict:
    """
    ì¿¼ë¦¬ ë³µì¡ë„ ë¶„ë¥˜
    
    ë¶„ë¥˜ ê¸°ì¤€:
    - simple: ì •ì˜, ë‹¨ìˆœ ì‚¬ì‹¤ ì§ˆë¬¸ â†’ ê²€ìƒ‰ ì—†ì´ ì§ì ‘ ë‹µë³€
    - moderate: ì¼ë°˜ì ì¸ ì •ë³´ ì§ˆë¬¸ â†’ ê¸°ë³¸ RAG
    - complex: ë¶„ì„, ë¹„êµ, ë‹¤ë‹¨ê³„ ì¶”ë¡  â†’ ê³ ê¸‰ RAG
    """
    print(f"\nğŸ” [ë¶„ë¥˜] ì¿¼ë¦¬ ë³µì¡ë„ ë¶„ì„ ì¤‘...")
    
    llm = get_llm()
    
    prompt = ChatPromptTemplate.from_messages([
        ("system", """ì§ˆë¬¸ì˜ ë³µì¡ë„ë¥¼ ë¶„ë¥˜í•˜ì„¸ìš”.

- simple: ê°„ë‹¨í•œ ì •ì˜, ë‹¨ìˆœ ì‚¬ì‹¤ ì§ˆë¬¸ (ì˜ˆ: "RAGê°€ ë­ì•¼?")
- moderate: ì¼ë°˜ì ì¸ ì •ë³´ ìš”ì²­ (ì˜ˆ: "RAGì˜ ì¥ì ì€?")
- complex: ë¶„ì„, ë¹„êµ, ë‹¤ë‹¨ê³„ ì¶”ë¡  í•„ìš” (ì˜ˆ: "RAGì™€ Fine-tuningì„ ë¹„êµí•´ì„œ ì–¸ì œ ë­˜ ì¨ì•¼ í• ì§€ ì„¤ëª…í•´ì¤˜")

"simple", "moderate", "complex" ì¤‘ í•˜ë‚˜ë§Œ ë‹µí•˜ì„¸ìš”."""),
        ("human", "ì§ˆë¬¸: {question}"),
    ])
    
    response = (prompt | llm).invoke({"question": state["question"]})
    
    content = response.content.lower().strip()
    if "complex" in content:
        complexity = "complex"
    elif "moderate" in content:
        complexity = "moderate"
    else:
        complexity = "simple"
    
    print(f"   â†’ ë³µì¡ë„: {complexity}")
    
    return {"query_complexity": complexity}


# =============================================================================
# 4. ì „ëµë³„ ë…¸ë“œ
# =============================================================================

def simple_strategy_node(state: AdaptiveRAGState) -> dict:
    """
    Simple ì „ëµ: ê²€ìƒ‰ ì—†ì´ ì§ì ‘ ë‹µë³€
    
    ê°„ë‹¨í•œ ì§ˆë¬¸ì€ LLMì˜ ê¸°ë³¸ ì§€ì‹ìœ¼ë¡œ ì¶©ë¶„íˆ ë‹µë³€ ê°€ëŠ¥.
    ê²€ìƒ‰ ë¹„ìš©ì„ ì ˆì•½í•©ë‹ˆë‹¤.
    """
    print("\nâš¡ [Simple ì „ëµ] ì§ì ‘ ë‹µë³€ ìƒì„±...")
    
    llm = get_llm()
    
    prompt = ChatPromptTemplate.from_messages([
        ("system", "ê°„ë‹¨í•˜ê³  ëª…í™•í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”."),
        ("human", "{question}"),
    ])
    
    response = (prompt | llm).invoke({"question": state["question"]})
    
    return {
        "strategy_used": "simple (ì§ì ‘ ë‹µë³€)",
        "answer": response.content
    }


def moderate_strategy_node(state: AdaptiveRAGState) -> dict:
    """
    Moderate ì „ëµ: ê¸°ë³¸ RAG
    
    ê²€ìƒ‰ + ìƒì„±ì˜ í‘œì¤€ RAG íŒŒì´í”„ë¼ì¸.
    """
    print("\nğŸ“š [Moderate ì „ëµ] ê¸°ë³¸ RAG ì‹¤í–‰...")
    
    # ê²€ìƒ‰
    vs = get_adaptive_vs()
    docs = vs.search(query=state["question"], k=3)
    
    context = "\n".join([doc.page_content for doc in docs])
    print(f"   â†’ {len(docs)}ê°œ ë¬¸ì„œ ê²€ìƒ‰ë¨")
    
    # ìƒì„±
    llm = get_llm()
    prompt = ChatPromptTemplate.from_messages([
        ("system", """ì»¨í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”.

ì»¨í…ìŠ¤íŠ¸:
{context}"""),
        ("human", "{question}"),
    ])
    
    response = (prompt | llm).invoke({
        "context": context,
        "question": state["question"]
    })
    
    return {
        "strategy_used": "moderate (ê¸°ë³¸ RAG)",
        "documents": docs,
        "context": context,
        "answer": response.content
    }


def complex_strategy_node(state: AdaptiveRAGState) -> dict:
    """
    Complex ì „ëµ: ê³ ê¸‰ RAG (ë‹¤ë‹¨ê³„ ì¶”ë¡ )
    
    1. ì§ˆë¬¸ ë¶„í•´
    2. ê° í•˜ìœ„ ì§ˆë¬¸ì— ëŒ€í•´ ê²€ìƒ‰
    3. í†µí•© ë‹µë³€ ìƒì„±
    """
    print("\nğŸ”¬ [Complex ì „ëµ] ê³ ê¸‰ RAG ì‹¤í–‰...")
    
    llm = get_llm()
    
    # 1ë‹¨ê³„: ì§ˆë¬¸ ë¶„í•´
    print("   [1/3] ì§ˆë¬¸ ë¶„í•´...")
    decompose_prompt = ChatPromptTemplate.from_messages([
        ("system", """ë³µì¡í•œ ì§ˆë¬¸ì„ 2-3ê°œì˜ í•˜ìœ„ ì§ˆë¬¸ìœ¼ë¡œ ë¶„í•´í•˜ì„¸ìš”.
ê° í•˜ìœ„ ì§ˆë¬¸ì€ í•œ ì¤„ì”© ì‘ì„±í•˜ì„¸ìš”."""),
        ("human", "{question}"),
    ])
    
    sub_questions_response = (decompose_prompt | llm).invoke({
        "question": state["question"]
    })
    sub_questions = [q.strip() for q in sub_questions_response.content.strip().split("\n") if q.strip()][:3]
    
    print(f"      â†’ í•˜ìœ„ ì§ˆë¬¸: {sub_questions}")
    
    # 2ë‹¨ê³„: ê° ì§ˆë¬¸ì— ëŒ€í•´ ê²€ìƒ‰
    print("   [2/3] í•˜ìœ„ ì§ˆë¬¸ë³„ ê²€ìƒ‰...")
    vs = get_adaptive_vs()
    all_docs = []
    seen = set()
    
    for sq in sub_questions:
        docs = vs.search(query=sq, k=2)
        for doc in docs:
            if doc.page_content not in seen:
                all_docs.append(doc)
                seen.add(doc.page_content)
    
    context = "\n\n".join([doc.page_content for doc in all_docs])
    print(f"      â†’ ì´ {len(all_docs)}ê°œ ë¬¸ì„œ")
    
    # 3ë‹¨ê³„: í†µí•© ë‹µë³€ ìƒì„±
    print("   [3/3] í†µí•© ë‹µë³€ ìƒì„±...")
    synthesize_prompt = ChatPromptTemplate.from_messages([
        ("system", """ë‹¤ìŒ í•˜ìœ„ ì§ˆë¬¸ë“¤ê³¼ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ 
ì›ë³¸ ì§ˆë¬¸ì— ëŒ€í•œ ì¢…í•©ì ì¸ ë‹µë³€ì„ ì‘ì„±í•˜ì„¸ìš”.

í•˜ìœ„ ì§ˆë¬¸ë“¤: {sub_questions}

ì»¨í…ìŠ¤íŠ¸:
{context}"""),
        ("human", "ì›ë³¸ ì§ˆë¬¸: {question}"),
    ])
    
    response = (synthesize_prompt | llm).invoke({
        "question": state["question"],
        "sub_questions": sub_questions,
        "context": context
    })
    
    return {
        "strategy_used": f"complex (ë‹¤ë‹¨ê³„ RAG, í•˜ìœ„ì§ˆë¬¸: {len(sub_questions)}ê°œ)",
        "documents": all_docs,
        "context": context,
        "answer": response.content
    }


# =============================================================================
# 5. ë¼ìš°í„°
# =============================================================================

def route_by_complexity(state: AdaptiveRAGState) -> Literal["simple", "moderate", "complex"]:
    """ë³µì¡ë„ì— ë”°ë¼ ì „ëµ ë¼ìš°íŒ…"""
    complexity = state.get("query_complexity", "moderate")
    print(f"ğŸ”€ ë¼ìš°íŒ…: {complexity} ì „ëµìœ¼ë¡œ ì´ë™")
    return complexity


# =============================================================================
# 6. ê·¸ë˜í”„ ìƒì„±
# =============================================================================

def create_adaptive_rag_graph():
    """
    Adaptive RAG ê·¸ë˜í”„
    
    êµ¬ì¡°:
        START â†’ classify â†’ (simple | moderate | complex) â†’ END
    """
    graph = StateGraph(AdaptiveRAGState)
    
    graph.add_node("classify", classify_query_node)
    graph.add_node("simple", simple_strategy_node)
    graph.add_node("moderate", moderate_strategy_node)
    graph.add_node("complex", complex_strategy_node)
    
    graph.add_edge(START, "classify")
    graph.add_conditional_edges(
        "classify",
        route_by_complexity,
        {
            "simple": "simple",
            "moderate": "moderate",
            "complex": "complex"
        }
    )
    graph.add_edge("simple", END)
    graph.add_edge("moderate", END)
    graph.add_edge("complex", END)
    
    print("âœ… Adaptive RAG ì»´íŒŒì¼ ì™„ë£Œ!")
    return graph.compile()


# =============================================================================
# 7. ì‹¤í–‰
# =============================================================================

def run_adaptive_rag(question: str) -> str:
    graph = create_adaptive_rag_graph()
    
    initial_state = {
        "question": question,
        "query_complexity": "",
        "strategy_used": "",
        "documents": [],
        "context": "",
        "answer": ""
    }
    
    print(f"\n{'='*60}")
    print(f"ğŸ™‹ ì§ˆë¬¸: {question}")
    print('='*60)
    
    result = graph.invoke(initial_state)
    
    print(f"\nğŸ“Š ì‚¬ìš©ëœ ì „ëµ: {result['strategy_used']}")
    print(f"\nğŸ¤– ë‹µë³€:\n{result['answer']}")
    print('='*60)
    
    return result["answer"]


if __name__ == "__main__":
    from utils.llm_factory import log_llm_error
    
    print("\n" + "="*60)
    print("Adaptive RAG ì˜ˆì œ")
    print("="*60)
    
    queries = [
        "RAGê°€ ë­ì•¼?",                              # simple
        "LangGraphì˜ ì£¼ìš” íŠ¹ì§•ì€?",                  # moderate
        "RAGì™€ Fine-tuningì„ ë¹„êµí•˜ê³  ê°ê° ì–¸ì œ ì‚¬ìš©í•´ì•¼ í• ì§€ ë¶„ì„í•´ì¤˜",  # complex
    ]
    
    for query in queries:
        try:
            run_adaptive_rag(query)
        except Exception as e:
            log_llm_error(e)
            print(f"âŒ ì˜¤ë¥˜: {e}")
        print()
