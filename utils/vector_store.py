# -*- coding: utf-8 -*-
"""
Vector Store ê´€ë¦¬ ëª¨ë“ˆ

ì´ ëª¨ë“ˆì€ RAG ì‹œìŠ¤í…œì—ì„œ ì‚¬ìš©í•˜ëŠ” Vector Storeë¥¼ ê´€ë¦¬í•©ë‹ˆë‹¤.
ChromaDBë¥¼ ê¸°ë³¸ ë°±ì—”ë“œë¡œ ì‚¬ìš©í•˜ë©°, ë¬¸ì„œ ì„ë² ë”© ë° ìœ ì‚¬ë„ ê²€ìƒ‰ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.

ì£¼ìš” ê¸°ëŠ¥:
    - ë¬¸ì„œ ë¡œë“œ ë° ì²­í‚¹
    - Vector Store ìƒì„± ë° ê´€ë¦¬
    - ìœ ì‚¬ë„ ê¸°ë°˜ ë¬¸ì„œ ê²€ìƒ‰

ì‚¬ìš© ì˜ˆì‹œ:
    from utils.vector_store import VectorStoreManager
    
    # Vector Store ë§¤ë‹ˆì € ìƒì„±
    manager = VectorStoreManager()
    
    # ë¬¸ì„œ ì¶”ê°€
    manager.add_documents(documents)
    
    # ê²€ìƒ‰
    results = manager.search("ì¿¼ë¦¬ í…ìŠ¤íŠ¸")
"""

import logging
from pathlib import Path
from typing import List, Optional, Any

from langchain_core.documents import Document
from langchain_core.embeddings import Embeddings
from langchain_core.vectorstores import VectorStore
from langchain_text_splitters import RecursiveCharacterTextSplitter

logger = logging.getLogger(__name__)


class VectorStoreManager:
    """
    Vector Storeë¥¼ ê´€ë¦¬í•˜ëŠ” í´ë˜ìŠ¤
    
    ì´ í´ë˜ìŠ¤ëŠ” ë¬¸ì„œë¥¼ ì²­í‚¹í•˜ê³ , ì„ë² ë”©í•˜ì—¬ Vector Storeì— ì €ì¥í•˜ëŠ”
    ì „ì²´ íŒŒì´í”„ë¼ì¸ì„ ê´€ë¦¬í•©ë‹ˆë‹¤.
    
    Attributes:
        embeddings: ì„ë² ë”© ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤
        vector_store: Vector Store ì¸ìŠ¤í„´ìŠ¤
        text_splitter: í…ìŠ¤íŠ¸ ë¶„í• ê¸°
        collection_name: ì»¬ë ‰ì…˜ ì´ë¦„
        persist_directory: ì˜êµ¬ ì €ì¥ ê²½ë¡œ
    
    Example:
        >>> from utils.llm_factory import get_embeddings
        >>> embeddings = get_embeddings()
        >>> manager = VectorStoreManager(embeddings=embeddings)
        >>> manager.add_texts(["ë¬¸ì„œ 1", "ë¬¸ì„œ 2"])
        >>> results = manager.search("ê²€ìƒ‰ì–´")
    """
    
    def __init__(
        self,
        embeddings: Optional[Embeddings] = None,
        collection_name: str = "langgraph_rag",
        persist_directory: Optional[str] = None,
        chunk_size: int = 1000,
        chunk_overlap: int = 200,
    ):
        """
        VectorStoreManager ì´ˆê¸°í™”
        
        Args:
            embeddings: ì„ë² ë”© ëª¨ë¸ (Noneì´ë©´ ìë™ ìƒì„±)
            collection_name: ChromaDB ì»¬ë ‰ì…˜ ì´ë¦„
            persist_directory: ì˜êµ¬ ì €ì¥ ê²½ë¡œ (Noneì´ë©´ ë©”ëª¨ë¦¬ì—ë§Œ ì €ì¥)
            chunk_size: í…ìŠ¤íŠ¸ ì²­í¬ í¬ê¸°
            chunk_overlap: ì²­í¬ ê°„ ì¤‘ë³µ í¬ê¸°
        """
        self.collection_name = collection_name
        self.persist_directory = persist_directory
        
        # ì„ë² ë”© ëª¨ë¸ ì„¤ì •
        if embeddings is None:
            from utils.llm_factory import get_embeddings
            self.embeddings = get_embeddings()
        else:
            self.embeddings = embeddings
        
        # í…ìŠ¤íŠ¸ ë¶„í• ê¸° ì„¤ì •
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            length_function=len,
            separators=["\n\n", "\n", ".", "!", "?", ",", " ", ""],
        )
        
        # Vector Store ì´ˆê¸°í™” (ì§€ì—° ì´ˆê¸°í™”)
        self._vector_store: Optional[VectorStore] = None
        
        logger.info(
            f"VectorStoreManager ì´ˆê¸°í™” ì™„ë£Œ "
            f"(ì»¬ë ‰ì…˜: {collection_name}, ì²­í¬ í¬ê¸°: {chunk_size})"
        )
    
    @property
    def vector_store(self) -> VectorStore:
        """
        Vector Store ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤ (ì§€ì—° ì´ˆê¸°í™”).
        
        Returns:
            VectorStore: ChromaDB Vector Store ì¸ìŠ¤í„´ìŠ¤
        """
        if self._vector_store is None:
            self._vector_store = self._create_vector_store()
        return self._vector_store
    
    def _create_vector_store(self) -> VectorStore:
        """
        ChromaDB Vector Storeë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        
        Returns:
            VectorStore: ìƒˆë¡œìš´ ChromaDB ì¸ìŠ¤í„´ìŠ¤
        """
        from langchain_chroma import Chroma
        
        logger.info("ChromaDB Vector Store ìƒì„± ì¤‘...")
        
        # ì˜êµ¬ ì €ì¥ ì„¤ì •
        persist_kwargs = {}
        if self.persist_directory:
            persist_kwargs["persist_directory"] = self.persist_directory
            # ë””ë ‰í† ë¦¬ ìƒì„±
            Path(self.persist_directory).mkdir(parents=True, exist_ok=True)
        
        vector_store = Chroma(
            collection_name=self.collection_name,
            embedding_function=self.embeddings,
            **persist_kwargs
        )
        
        logger.info("ChromaDB Vector Store ìƒì„± ì™„ë£Œ")
        return vector_store
    
    def split_text(self, text: str) -> List[str]:
        """
        í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• í•©ë‹ˆë‹¤.
        
        Args:
            text: ë¶„í• í•  í…ìŠ¤íŠ¸
        
        Returns:
            List[str]: ë¶„í• ëœ í…ìŠ¤íŠ¸ ì²­í¬ ë¦¬ìŠ¤íŠ¸
        
        Example:
            >>> chunks = manager.split_text("ê¸´ í…ìŠ¤íŠ¸...")
            >>> print(f"ìƒì„±ëœ ì²­í¬ ìˆ˜: {len(chunks)}")
        """
        chunks = self.text_splitter.split_text(text)
        logger.info(f"í…ìŠ¤íŠ¸ë¥¼ {len(chunks)}ê°œì˜ ì²­í¬ë¡œ ë¶„í• í–ˆìŠµë‹ˆë‹¤.")
        return chunks
    
    def split_documents(self, documents: List[Document]) -> List[Document]:
        """
        Document ê°ì²´ë“¤ì„ ì²­í¬ë¡œ ë¶„í• í•©ë‹ˆë‹¤.
        
        Args:
            documents: ë¶„í• í•  Document ë¦¬ìŠ¤íŠ¸
        
        Returns:
            List[Document]: ë¶„í• ëœ Document ë¦¬ìŠ¤íŠ¸
        """
        chunks = self.text_splitter.split_documents(documents)
        logger.info(
            f"{len(documents)}ê°œì˜ ë¬¸ì„œë¥¼ {len(chunks)}ê°œì˜ ì²­í¬ë¡œ ë¶„í• í–ˆìŠµë‹ˆë‹¤."
        )
        return chunks
    
    def add_texts(
        self,
        texts: List[str],
        metadatas: Optional[List[dict]] = None,
    ) -> List[str]:
        """
        í…ìŠ¤íŠ¸ë¥¼ Vector Storeì— ì¶”ê°€í•©ë‹ˆë‹¤.
        
        Args:
            texts: ì¶”ê°€í•  í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
            metadatas: ê° í…ìŠ¤íŠ¸ì˜ ë©”íƒ€ë°ì´í„° (ì„ íƒ)
        
        Returns:
            List[str]: ì¶”ê°€ëœ ë¬¸ì„œì˜ ID ë¦¬ìŠ¤íŠ¸
        
        Example:
            >>> ids = manager.add_texts(
            ...     texts=["ë¬¸ì„œ 1", "ë¬¸ì„œ 2"],
            ...     metadatas=[{"source": "doc1"}, {"source": "doc2"}]
            ... )
        """
        # ì„ë² ë”© ìš”ì²­ ì§ì „ ìƒì„¸ ë¡œê·¸ (ë””ë²„ê¹…ìš©)
        print(f"\nğŸ“¤ ì„ë² ë”© ëª¨ë¸ë¡œ ìš”ì²­ ì¤€ë¹„ ì¤‘...")
        print(f"   - ìš”ì²­í•  í…ìŠ¤íŠ¸ ìˆ˜: {len(texts)}ê°œ")
        print(f"   - ì„ë² ë”© ëª¨ë¸ íƒ€ì…: {type(self.embeddings).__name__}")
        print(f"   - ì„ë² ë”© ëª¨ë¸ ì •ë³´: {self.embeddings}")
        
        # ì²« ë²ˆì§¸ í…ìŠ¤íŠ¸ì˜ ë¯¸ë¦¬ë³´ê¸° (ë””ë²„ê¹…ìš©)
        if texts:
            preview = texts[0][:100].replace('\n', ' ')
            print(f"   - ì²« ë²ˆì§¸ í…ìŠ¤íŠ¸ ë¯¸ë¦¬ë³´ê¸°: {preview}...")
        
        logger.info(f"{len(texts)}ê°œì˜ í…ìŠ¤íŠ¸ë¥¼ Vector Storeì— ì¶”ê°€ ì¤‘...")
        
        try:
            print("   â³ ì„ë² ë”© ëª¨ë¸ë¡œ ë²¡í„°í™” ìš”ì²­ ì¤‘... (ì„œë²„ ì‘ë‹µ ëŒ€ê¸°)")
            ids = self.vector_store.add_texts(texts=texts, metadatas=metadatas)
            print(f"   âœ… ì„ë² ë”© ì™„ë£Œ! {len(ids)}ê°œì˜ ë²¡í„°ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
        except Exception as e:
            print(f"\nâŒ ì„ë² ë”© ì¤‘ ì˜¤ë¥˜ ë°œìƒ!")
            print(f"   ì˜¤ë¥˜ íƒ€ì…: {type(e).__name__}")
            print(f"   ì˜¤ë¥˜ ë©”ì‹œì§€: {str(e)}")
            raise  # ì˜¤ë¥˜ë¥¼ ë‹¤ì‹œ ë˜ì ¸ì„œ ìƒìœ„ì—ì„œ ì²˜ë¦¬í•˜ë„ë¡ í•¨
        
        logger.info(f"{len(ids)}ê°œì˜ í…ìŠ¤íŠ¸ê°€ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return ids
    
    def add_documents(
        self,
        documents: List[Document],
        split: bool = True,
    ) -> List[str]:
        """
        Document ê°ì²´ë“¤ì„ Vector Storeì— ì¶”ê°€í•©ë‹ˆë‹¤.
        
        Args:
            documents: ì¶”ê°€í•  Document ë¦¬ìŠ¤íŠ¸
            split: ë¬¸ì„œë¥¼ ì²­í¬ë¡œ ë¶„í• í• ì§€ ì—¬ë¶€ (ê¸°ë³¸ê°’: True)
        
        Returns:
            List[str]: ì¶”ê°€ëœ ë¬¸ì„œì˜ ID ë¦¬ìŠ¤íŠ¸
        """
        if split:
            documents = self.split_documents(documents)
        
        # ì„ë² ë”© ìš”ì²­ ì§ì „ ìƒì„¸ ë¡œê·¸ (ë””ë²„ê¹…ìš©)
        print(f"\nğŸ“¤ ì„ë² ë”© ëª¨ë¸ë¡œ ë¬¸ì„œ ë²¡í„°í™” ìš”ì²­ ì¤€ë¹„ ì¤‘...")
        print(f"   - ìš”ì²­í•  ë¬¸ì„œ ìˆ˜: {len(documents)}ê°œ")
        print(f"   - ì„ë² ë”© ëª¨ë¸ íƒ€ì…: {type(self.embeddings).__name__}")
        
        logger.info(f"{len(documents)}ê°œì˜ ë¬¸ì„œë¥¼ Vector Storeì— ì¶”ê°€ ì¤‘...")
        
        try:
            print("   â³ ì„ë² ë”© ëª¨ë¸ë¡œ ë²¡í„°í™” ìš”ì²­ ì¤‘... (ì„œë²„ ì‘ë‹µ ëŒ€ê¸°)")
            ids = self.vector_store.add_documents(documents=documents)
            print(f"   âœ… ì„ë² ë”© ì™„ë£Œ! {len(ids)}ê°œì˜ ë²¡í„°ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
        except Exception as e:
            print(f"\nâŒ ë¬¸ì„œ ì„ë² ë”© ì¤‘ ì˜¤ë¥˜ ë°œìƒ!")
            print(f"   ì˜¤ë¥˜ íƒ€ì…: {type(e).__name__}")
            print(f"   ì˜¤ë¥˜ ë©”ì‹œì§€: {str(e)}")
            raise  # ì˜¤ë¥˜ë¥¼ ë‹¤ì‹œ ë˜ì ¸ì„œ ìƒìœ„ì—ì„œ ì²˜ë¦¬í•˜ë„ë¡ í•¨
        
        logger.info(f"{len(ids)}ê°œì˜ ë¬¸ì„œê°€ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return ids
    
    def search(
        self,
        query: str,
        k: int = 4,
        filter: Optional[dict] = None,
    ) -> List[Document]:
        """
        ìœ ì‚¬ë„ ê¸°ë°˜ìœ¼ë¡œ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            k: ë°˜í™˜í•  ë¬¸ì„œ ìˆ˜ (ê¸°ë³¸ê°’: 4)
            filter: ë©”íƒ€ë°ì´í„° í•„í„° (ì„ íƒ)
        
        Returns:
            List[Document]: ê²€ìƒ‰ëœ Document ë¦¬ìŠ¤íŠ¸
        
        Example:
            >>> results = manager.search("LangGraphë€?", k=3)
            >>> for doc in results:
            ...     print(doc.page_content[:100])
        """
        logger.info(f"ê²€ìƒ‰ ì¤‘: '{query}' (k={k})")
        
        results = self.vector_store.similarity_search(
            query=query,
            k=k,
            filter=filter,
        )
        
        logger.info(f"{len(results)}ê°œì˜ ë¬¸ì„œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
        return results
    
    def search_with_score(
        self,
        query: str,
        k: int = 4,
    ) -> List[tuple[Document, float]]:
        """
        ìœ ì‚¬ë„ ì ìˆ˜ì™€ í•¨ê»˜ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            k: ë°˜í™˜í•  ë¬¸ì„œ ìˆ˜
        
        Returns:
            List[tuple[Document, float]]: (Document, ìœ ì‚¬ë„ ì ìˆ˜) íŠœí”Œ ë¦¬ìŠ¤íŠ¸
        """
        logger.info(f"ì ìˆ˜ í¬í•¨ ê²€ìƒ‰ ì¤‘: '{query}' (k={k})")
        
        results = self.vector_store.similarity_search_with_score(
            query=query,
            k=k,
        )
        
        logger.info(f"{len(results)}ê°œì˜ ë¬¸ì„œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
        return results
    
    def as_retriever(self, **kwargs) -> Any:
        """
        Vector Storeë¥¼ Retrieverë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
        
        LangChainì˜ LCEL ì²´ì¸ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” Retrieverë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        
        Args:
            **kwargs: Retriever ì„¤ì • (search_type, search_kwargs ë“±)
        
        Returns:
            VectorStoreRetriever: Retriever ì¸ìŠ¤í„´ìŠ¤
        
        Example:
            >>> retriever = manager.as_retriever(
            ...     search_type="similarity",
            ...     search_kwargs={"k": 4}
            ... )
        """
        return self.vector_store.as_retriever(**kwargs)
    
    def load_from_file(
        self,
        file_path: str,
        encoding: str = "utf-8",
    ) -> List[str]:
        """
        í…ìŠ¤íŠ¸ íŒŒì¼ì—ì„œ ë¬¸ì„œë¥¼ ë¡œë“œí•˜ê³  Vector Storeì— ì¶”ê°€í•©ë‹ˆë‹¤.
        
        Args:
            file_path: íŒŒì¼ ê²½ë¡œ
            encoding: íŒŒì¼ ì¸ì½”ë”© (ê¸°ë³¸ê°’: utf-8)
        
        Returns:
            List[str]: ì¶”ê°€ëœ ë¬¸ì„œì˜ ID ë¦¬ìŠ¤íŠ¸
        """
        logger.info(f"íŒŒì¼ ë¡œë“œ ì¤‘: {file_path}")
        
        with open(file_path, "r", encoding=encoding) as f:
            content = f.read()
        
        # ì²­í‚¹ ë° ì¶”ê°€
        chunks = self.split_text(content)
        
        # ë©”íƒ€ë°ì´í„°ì— ì†ŒìŠ¤ íŒŒì¼ ì •ë³´ ì¶”ê°€
        metadatas = [{"source": file_path} for _ in chunks]
        
        return self.add_texts(texts=chunks, metadatas=metadatas)
    
    def clear(self):
        """
        Vector Storeì˜ ëª¨ë“  ë¬¸ì„œë¥¼ ì‚­ì œí•©ë‹ˆë‹¤.
        
        ì£¼ì˜: ì´ ì‘ì—…ì€ ë˜ëŒë¦´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.
        """
        logger.warning("Vector Storeì˜ ëª¨ë“  ë¬¸ì„œë¥¼ ì‚­ì œí•©ë‹ˆë‹¤.")
        
        # ìƒˆ Vector Store ìƒì„±ìœ¼ë¡œ ì´ˆê¸°í™”
        self._vector_store = self._create_vector_store()
        
        logger.info("Vector Storeê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")


# í…ŒìŠ¤íŠ¸ìš© ì½”ë“œ
if __name__ == "__main__":
    # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸
    print("VectorStoreManager í…ŒìŠ¤íŠ¸")
    
    # ğŸ’¡ get_embeddings()ëŠ” ë‚´ë¶€ì ìœ¼ë¡œ .envì˜ EMBEDDING_PROVIDER ì„¤ì •ì„ ë”°ë¦…ë‹ˆë‹¤.
    # íŠ¹ì • providerë¥¼ ê°•ì œí•˜ê³  ì‹¶ë‹¤ë©´ get_embeddings(provider="openai") ì²˜ëŸ¼ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.
    try:
        from utils.llm_factory import get_embeddings
        # ê¸°ë³¸ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ (OpenAI ë˜ëŠ” Ollama)
        embeddings = get_embeddings()
        
        manager = VectorStoreManager(embeddings=embeddings, collection_name="test_collection")
        
        # í…ŒìŠ¤íŠ¸ ë¬¸ì„œ ì¶”ê°€
        test_texts = [
            "LangGraphëŠ” LangChain ìœ„ì— êµ¬ì¶•ëœ ìƒíƒœ ê¸°ë°˜ ì—ì´ì „íŠ¸ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.",
            "RAGëŠ” Retrieval-Augmented Generationì˜ ì•½ìë¡œ, ê²€ìƒ‰ ì¦ê°• ìƒì„±ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.",
            "Vector StoreëŠ” ì„ë² ë”© ë²¡í„°ë¥¼ ì €ì¥í•˜ê³  ìœ ì‚¬ë„ ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.",
        ]
        
        print("\nğŸ“¥ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¶”ê°€ ì¤‘...")
        manager.add_texts(test_texts)
        
        # ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
        query = "LangGraphë€ ë¬´ì—‡ì¸ê°€ìš”?"
        results = manager.search(query, k=2)
        
        print(f"\nğŸ” ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ (ì¿¼ë¦¬: '{query}')")
        print(f"ê²°ê³¼ ({len(results)}ê°œ):")
        for i, doc in enumerate(results, 1):
            print(f"   [{i}] {doc.page_content[:100]}")
            
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
