# -*- coding: utf-8 -*-
"""
LLM íŒ©í† ë¦¬ ëª¨ë“ˆ

ì´ ëª¨ë“ˆì€ ë‹¤ì–‘í•œ LLM ì œê³µì(OpenAI, Ollama ë“±)ë¥¼ ìœ„í•œ 
íŒ©í† ë¦¬ íŒ¨í„´ì„ êµ¬í˜„í•©ë‹ˆë‹¤. ì„¤ì •ì— ë”°ë¼ ì ì ˆí•œ LLM ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

ì£¼ìš” ê¸°ëŠ¥:
    - OpenAI ChatGPT ëª¨ë¸ ìƒì„±
    - Ollama ë¡œì»¬ ëª¨ë¸ ìƒì„± (ì„ íƒ)
    - ì„ë² ë”© ëª¨ë¸ ìƒì„±

ì‚¬ìš© ì˜ˆì‹œ:
    from utils.llm_factory import get_llm, get_embeddings
    
    # LLM ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    llm = get_llm()
    
    # ì„ë² ë”© ëª¨ë¸ ìƒì„±
    embeddings = get_embeddings()
"""

import logging
from typing import Optional

from langchain_core.language_models import BaseChatModel
from langchain_core.embeddings import Embeddings

logger = logging.getLogger(__name__)


class LLMFactory:
    """
    LLM ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±í•˜ëŠ” íŒ©í† ë¦¬ í´ë˜ìŠ¤
    
    ì´ í´ë˜ìŠ¤ëŠ” ì„¤ì •ì— ë”°ë¼ OpenAI ë˜ëŠ” Ollama LLMì„ ìƒì„±í•©ë‹ˆë‹¤.
    íŒ©í† ë¦¬ íŒ¨í„´ì„ ì‚¬ìš©í•˜ì—¬ LLM ìƒì„± ë¡œì§ì„ ìº¡ìŠí™”í•©ë‹ˆë‹¤.
    
    Attributes:
        _llm_cache: ìƒì„±ëœ LLM ì¸ìŠ¤í„´ìŠ¤ ìºì‹œ (ì‹±ê¸€í†¤ íŒ¨í„´)
        _embeddings_cache: ìƒì„±ëœ ì„ë² ë”© ì¸ìŠ¤í„´ìŠ¤ ìºì‹œ
    """
    
    _llm_cache: Optional[BaseChatModel] = None
    _embeddings_cache: Optional[Embeddings] = None
    
    @classmethod
    def create_openai_llm(
        cls,
        api_key: str,
        model: str = "gpt-4o-mini",
        temperature: float = 0.0,
        base_url: Optional[str] = None,
        **kwargs
    ) -> BaseChatModel:
        """
        OpenAI ChatGPT ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        
        Args:
            api_key: OpenAI API í‚¤
            model: ì‚¬ìš©í•  ëª¨ë¸ëª… (ê¸°ë³¸ê°’: gpt-4o-mini)
            temperature: ìƒì„± ì˜¨ë„ (0.0 = ê²°ì •ì , 1.0 = ì°½ì˜ì )
            **kwargs: ì¶”ê°€ ë§¤ê°œë³€ìˆ˜
        
        Returns:
            BaseChatModel: OpenAI ChatGPT ì¸ìŠ¤í„´ìŠ¤
        
        Raises:
            ValueError: API í‚¤ê°€ ìœ íš¨í•˜ì§€ ì•Šì€ ê²½ìš°
        
        Example:
            >>> llm = LLMFactory.create_openai_llm(
            ...     api_key="sk-...",
            ...     model="gpt-4o-mini"
            ... )
        """
        if not api_key:
             # ë¡œì»¬ LLMì„ ìœ„í•´ ë”ë¯¸ í‚¤ë¥¼ í—ˆìš©í•˜ì§€ë§Œ ê²½ê³ ë¥¼ ë‚¨ê¸¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
             # ì—¬ê¸°ì„œëŠ” í˜¸ì¶œìê°€ ì²˜ë¦¬í–ˆë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤.
             pass
        
        logger.info(f"OpenAI í˜¸í™˜ LLM ìƒì„± ì¤‘... (ëª¨ë¸: {model}, URL: {base_url or 'default'})")
        
        from langchain_openai import ChatOpenAI
        
        llm = ChatOpenAI(
            api_key=api_key or "dummy-key", # API key is required by library, use dummy if empty
            model=model,
            temperature=temperature,
            base_url=base_url,
            **kwargs
        )
        
        logger.info("OpenAI í˜¸í™˜ LLM ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì™„ë£Œ")
        return llm
    
    @classmethod
    def create_openai_embeddings(
        cls,
        api_key: str,
        model: str = "text-embedding-3-small",
        **kwargs
    ) -> Embeddings:
        """
        OpenAI ì„ë² ë”© ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        
        Args:
            api_key: OpenAI API í‚¤
            model: ì„ë² ë”© ëª¨ë¸ëª… (ê¸°ë³¸ê°’: text-embedding-3-small)
            **kwargs: ì¶”ê°€ ë§¤ê°œë³€ìˆ˜
        
        Returns:
            Embeddings: OpenAI ì„ë² ë”© ì¸ìŠ¤í„´ìŠ¤
        
        Example:
            >>> embeddings = LLMFactory.create_openai_embeddings(
            ...     api_key="sk-...",
            ...     model="text-embedding-3-small"
            ... )
        """
        # ë¡œì»¬ LLMì˜ ì„ë² ë”©ì„ ì‚¬ìš©í•˜ëŠ” ê²½ìš° ì‹œì‘ ë¬¸ìì—´ ê²€ì¦ ë¡œì§ì€ ì œê±°í•˜ê±°ë‚˜ ì™„í™”í•´ì•¼ í•©ë‹ˆë‹¤.
        
        logger.info(f"OpenAI ì„ë² ë”© ëª¨ë¸ ìƒì„± ì¤‘... (ëª¨ë¸: {model})")
        
        from langchain_openai import OpenAIEmbeddings
        
        embeddings = OpenAIEmbeddings(
            api_key=api_key or "dummy-key",
            model=model,
            base_url=kwargs.pop("base_url", None),
            **kwargs
        )
        
        logger.info("OpenAI ì„ë² ë”© ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì™„ë£Œ")
        return embeddings


def get_llm(**kwargs) -> BaseChatModel:
    """
    ì„¤ì •ì— ë”°ë¼ ì ì ˆí•œ LLM ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    
    ì´ í•¨ìˆ˜ëŠ” ìºì‹œë¥¼ ì‚¬ìš©í•˜ì—¬ ë™ì¼í•œ LLM ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì¬ì‚¬ìš©í•©ë‹ˆë‹¤.
    
    Args:
        **kwargs: LLM ìƒì„±ì— ì „ë‹¬í•  ì¶”ê°€ ë§¤ê°œë³€ìˆ˜
    
    Returns:
        BaseChatModel: LLM ì¸ìŠ¤í„´ìŠ¤
    
    Example:
        >>> llm = get_llm()
        >>> response = llm.invoke("ì•ˆë…•í•˜ì„¸ìš”!")
    """
    # ì„¤ì • ë¡œë“œ
    from config.settings import get_settings
    settings = get_settings()
    
    # LLM ìƒì„± (ë¬´ì¡°ê±´ OpenAI í˜¸í™˜ ì‚¬ìš©)
    return LLMFactory.create_openai_llm(
        api_key=settings.openai_api_key,
        model=settings.openai_model,
        base_url=settings.openai_api_base,
        **kwargs
    )


def get_embeddings(**kwargs) -> Embeddings:
    """
    ì„¤ì •ì— ë”°ë¼ ì ì ˆí•œ ì„ë² ë”© ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    
    Args:
        **kwargs: ì„ë² ë”© ìƒì„±ì— ì „ë‹¬í•  ì¶”ê°€ ë§¤ê°œë³€ìˆ˜
    
    Returns:
        Embeddings: ì„ë² ë”© ì¸ìŠ¤í„´ìŠ¤
    
    Example:
        >>> embeddings = get_embeddings()
        >>> vector = embeddings.embed_query("ì•ˆë…•í•˜ì„¸ìš”")
    """
    # ì„¤ì • ë¡œë“œ
    from config.settings import get_settings
    settings = get_settings()
    
    # ì„ë² ë”© ëª¨ë¸ ìƒì„± (ë¬´ì¡°ê±´ OpenAI í˜¸í™˜ ì‚¬ìš©)
    # ì„ë² ë”© ì „ìš© URLì´ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ê¸°ë³¸ API Base URL ì‚¬ìš©
    embedding_base_url = settings.openai_embedding_api_base or settings.openai_api_base
    
    return LLMFactory.create_openai_embeddings(
        api_key=settings.openai_api_key,
        model=settings.openai_embedding_model,
        base_url=embedding_base_url,
        **kwargs
    )


def log_llm_error(e: Exception, llm: Optional[BaseChatModel] = None):
    """
    LLM ê´€ë ¨ ì˜¤ë¥˜ ë°œìƒ ì‹œ ìƒì„¸í•œ ì •ë³´ë¥¼ ë¡œê¹…í•©ë‹ˆë‹¤.
    
    Args:
        e: ë°œìƒí•œ ì˜ˆì™¸ ê°ì²´
        llm: (ì„ íƒ) ê´€ë ¨ LLM ì¸ìŠ¤í„´ìŠ¤ (URL ì •ë³´ ì¶”ì¶œìš©)
    """
    import httpx
    import openai
    
    error_type = type(e).__name__
    base_url = "unknown"
    if llm:
        base_url = getattr(llm, "openai_api_base", "unknown")
    
    logger.error(f"âŒ LLM ì˜¤ë¥˜ ë°œìƒ! (Type: {error_type})")
    if llm:
        logger.error(f"ğŸ“ Target URL: {base_url}")
    
    if isinstance(e, openai.APIConnectionError):
        logger.error(f"ğŸ’¡ ì›ì¸: ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. URLì´ ì˜¬ë°”ë¥¸ì§€ í™•ì¸í•˜ì„¸ìš”.")
        logger.error(f"ğŸ‘‰ ìƒì„¸: {str(e)}")
    elif isinstance(e, httpx.ConnectError):
        logger.error(f"ğŸ’¡ ì›ì¸: ë„¤íŠ¸ì›Œí¬ ì—°ê²° ê±°ë¶€ë¨. ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•˜ì„¸ìš”.")
    elif isinstance(e, openai.AuthenticationError):
        logger.error(f"ğŸ’¡ ì›ì¸: ì¸ì¦ ì‹¤íŒ¨. API Keyê°€ ì˜¬ë°”ë¥¸ì§€ í™•ì¸í•˜ì„¸ìš”.")
    elif isinstance(e, openai.BadRequestError):
        logger.error(f"ğŸ’¡ ì›ì¸: ì˜ëª»ëœ ìš”ì²­ì…ë‹ˆë‹¤. ëª¨ë¸ëª…ì´ë‚˜ íŒŒë¼ë¯¸í„°ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        logger.error(f"ğŸ‘‰ ìƒì„¸: {str(e)}")
    else:
        logger.error(f"âš ï¸ ê¸°íƒ€ ì˜¤ë¥˜: {str(e)}")


# í…ŒìŠ¤íŠ¸ìš© ì½”ë“œ
if __name__ == "__main__":
    # ì„¤ì • ë¡œë“œ í…ŒìŠ¤íŠ¸
    from config.settings import get_settings
    settings = get_settings()
    
    print(f"LLM Base URL: {settings.openai_api_base}")
    
    llm = get_llm()
    print(f"LLM Type: {type(llm).__name__}")
