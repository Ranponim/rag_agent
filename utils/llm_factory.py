# -*- coding: utf-8 -*-
"""
LLM íŒ©í† ë¦¬ ëª¨ë“ˆ

ì´ ëª¨ë“ˆì€ ë‹¤ì–‘í•œ LLM ì œê³µì(OpenAI, Ollama ë“±)ë¥¼ ìœ„í•œ 
íŒ©í† ë¦¬ íŒ¨í„´ì„ êµ¬í˜„í•©ë‹ˆë‹¤. ì„¤ì •ì— ë”°ë¼ ì ì ˆí•œ LLM ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

ì£¼ìš” ê¸°ëŠ¥:
    - OpenAI ChatGPT ëª¨ë¸ ìƒì„± (Local LLM í˜¸í™˜)
    - ì„ë² ë”© ëª¨ë¸ ìƒì„±
    - ì‹±ê¸€í†¤ ìºì‹± ì§€ì›
"""

import logging
import os
from typing import Optional

from dotenv import load_dotenv
from langchain_core.language_models import BaseChatModel
from langchain_core.embeddings import Embeddings

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

logger = logging.getLogger(__name__)


class LLMFactory:
    """
    LLM ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±í•˜ëŠ” íŒ©í† ë¦¬ í´ë˜ìŠ¤
    """
    
    _llm_cache: Optional[BaseChatModel] = None
    _embeddings_cache: dict[str, Embeddings] = {}
    
    @classmethod
    def create_openai_llm(
        cls,
        api_key: str,
        model: str = "gpt-4o-mini",
        temperature: float = 0.0,
        base_url: Optional[str] = None,
        **kwargs
    ) -> BaseChatModel:
        """OpenAI ChatGPT ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±"""
        
        # ìºì‹œ í™•ì¸ (kwargsê°€ ì—†ì„ ë•Œë§Œ ìºì‹œ ì‚¬ìš©)
        if not kwargs and cls._llm_cache is not None:
            return cls._llm_cache

        logger.info(f"LLM ìƒì„± ì¤‘... (ëª¨ë¸: {model}, URL: {base_url})")
        
        from langchain_openai import ChatOpenAI
        
        llm = ChatOpenAI(
            api_key=api_key or "dummy-key",
            model=model,
            temperature=temperature,
            base_url=base_url,
            **kwargs
        )
        
        # ìºì‹œ ì €ì¥ (kwargsê°€ ì—†ì„ ë•Œë§Œ)
        if not kwargs:
            cls._llm_cache = llm

        return llm
    
    @classmethod
    def create_openai_embeddings(
        cls,
        api_key: str,
        model: str = "text-embedding-3-small",
        **kwargs
    ) -> Embeddings:
        """OpenAI ì„ë² ë”© ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±"""
        
        # ìºì‹œ í‚¤ ìƒì„±
        cache_key = f"openai_{model}"
        
        if not kwargs and cache_key in cls._embeddings_cache:
            return cls._embeddings_cache[cache_key]

        logger.info(f"OpenAI ì„ë² ë”© ëª¨ë¸ ìƒì„± ì¤‘... (ëª¨ë¸: {model})")
        
        from langchain_openai import OpenAIEmbeddings
        
        embeddings = OpenAIEmbeddings(
            api_key=api_key or "dummy-key",
            model=model,
            base_url=kwargs.pop("base_url", None),
            **kwargs
        )
        
        if not kwargs:
            cls._embeddings_cache[cache_key] = embeddings

        return embeddings
    
    @classmethod
    def create_ollama_embeddings(
        cls,
        model: str = "nomic-embed-text",
        base_url: Optional[str] = None,
        **kwargs
    ) -> Embeddings:
        """Ollama ì„ë² ë”© ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        
        Args:
            model: Ollama ì„ë² ë”© ëª¨ë¸ëª… (ì˜ˆ: nomic-embed-text, mxbai-embed-large)
            base_url: Ollama ì„œë²„ URL (ê¸°ë³¸ê°’: http://localhost:11434)
            **kwargs: ì¶”ê°€ íŒŒë¼ë¯¸í„°
            
        Returns:
            Embeddings: Ollama ì„ë² ë”© ì¸ìŠ¤í„´ìŠ¤
        """
        
        # ìºì‹œ í‚¤ ìƒì„±
        cache_key = f"ollama_{model}"
        
        # ìºì‹œ í™•ì¸ (kwargsê°€ ì—†ì„ ë•Œë§Œ ìºì‹œ ì‚¬ìš©)
        if not kwargs and cache_key in cls._embeddings_cache:
            return cls._embeddings_cache[cache_key]

        logger.info(f"Ollama ì„ë² ë”© ëª¨ë¸ ìƒì„± ì¤‘... (ëª¨ë¸: {model}, URL: {base_url})")
        
        from langchain_ollama import OllamaEmbeddings
        
        # OllamaEmbeddings ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        # base_urlì´ Noneì´ë©´ OllamaEmbeddingsì˜ ê¸°ë³¸ê°’(http://localhost:11434) ì‚¬ìš©
        embeddings = OllamaEmbeddings(
            model=model,
            base_url=base_url,
            **kwargs
        )
        
        # ìºì‹œ ì €ì¥ (kwargsê°€ ì—†ì„ ë•Œë§Œ)
        if not kwargs:
            cls._embeddings_cache[cache_key] = embeddings

        return embeddings


def get_llm(**kwargs) -> BaseChatModel:
    """LLM ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜ (ì‹±ê¸€í†¤)"""
    # í™˜ê²½ë³€ìˆ˜ì—ì„œ LLM ì„¤ì • ë¡œë“œ
    api_key = os.getenv("OPENAI_API_KEY", "lm-studio")
    model = os.getenv("OPENAI_MODEL", "local-model")
    api_base = os.getenv("OPENAI_API_BASE", "http://localhost:1234/v1")
    
    return LLMFactory.create_openai_llm(
        api_key=api_key,
        model=model,
        base_url=api_base,
        **kwargs
    )


def get_embeddings(**kwargs) -> Embeddings:
    """ì„ë² ë”© ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜ (ì‹±ê¸€í†¤)
    
    í™˜ê²½ë³€ìˆ˜ EMBEDDING_PROVIDERì— ë”°ë¼ OpenAI ë˜ëŠ” Ollama ì„ë² ë”©ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    
    Args:
        **kwargs: ì„ë² ë”© ëª¨ë¸ ìƒì„± ì‹œ ì „ë‹¬í•  ì¶”ê°€ íŒŒë¼ë¯¸í„°
        
    Returns:
        Embeddings: ì„¤ì •ëœ providerì— ë§ëŠ” ì„ë² ë”© ì¸ìŠ¤í„´ìŠ¤
        
    Example:
        >>> embeddings = get_embeddings()  # EMBEDDING_PROVIDERì— ë”°ë¼ ìë™ ì„ íƒ
        >>> vectors = embeddings.embed_documents(["Hello", "World"])
    """
    # ì¸ìë¡œ ë„˜ì–´ì˜¨ providerë¥¼ ìš°ì„ í•˜ê³  ì—†ìœ¼ë©´ í™˜ê²½ë³€ìˆ˜ ì‚¬ìš©
    provider = kwargs.pop("provider", os.getenv("EMBEDDING_PROVIDER", "openai")).lower()
    
    if provider == "ollama":
        # Ollama ì„ë² ë”© ì‚¬ìš© (ë¡œì»¬ ê¸°ë³¸ ì„¤ì • ì‚¬ìš©)
        model = os.getenv("OLLAMA_EMBEDDING_MODEL", "nomic-embed-text")
        
        logger.info(f"Ollama ì„ë² ë”© provider ì‚¬ìš© (ëª¨ë¸: {model}, ë¡œì»¬ ê¸°ë³¸ ì„¤ì •)")
        return LLMFactory.create_ollama_embeddings(
            model=model,
            **kwargs
        )
    else:
        # OpenAI ì„ë² ë”© ì‚¬ìš© (ê¸°ë³¸ê°’)
        api_key = os.getenv("OPENAI_API_KEY", "lm-studio")
        model = os.getenv("OPENAI_EMBEDDING_MODEL", "local-embedding-model")
        embedding_base = os.getenv("OPENAI_EMBEDDING_API_BASE") or os.getenv("OPENAI_API_BASE", "http://localhost:1234/v1")
        
        logger.info(f"OpenAI ì„ë² ë”© provider ì‚¬ìš© (ëª¨ë¸: {model})")
        return LLMFactory.create_openai_embeddings(
            api_key=api_key,
            model=model,
            base_url=embedding_base,
            **kwargs
        )


def log_llm_error(e: Exception):
    """LLM ê´€ë ¨ ì˜¤ë¥˜ ìƒì„¸ ë¡œê¹…
    
    Args:
        e: ë°œìƒí•œ ì˜ˆì™¸ ê°ì²´
        
    ì—ëŸ¬ íƒ€ì…ë³„ë¡œ ìƒì„¸í•œ ì§„ë‹¨ ì •ë³´ë¥¼ ì œê³µí•˜ì—¬ ë””ë²„ê¹…ì„ ë•ìŠµë‹ˆë‹¤.
    """
    import traceback
    
    # ë™ì  ì„í¬íŠ¸ (ëª¨ë“ˆì´ ì—†ì„ ìˆ˜ë„ ìˆìœ¼ë¯€ë¡œ try-exceptë¡œ ë³´í˜¸)
    try:
        import openai
    except ImportError:
        openai = None
    
    try:
        import httpx
    except ImportError:
        httpx = None
    
    # =========================================================================
    # 1. ê¸°ë³¸ ì˜¤ë¥˜ ì •ë³´ ì¶œë ¥
    # =========================================================================
    error_type = type(e).__name__
    error_module = type(e).__module__
    
    print("\n" + "="*80)
    print("âŒ LLM ì˜¤ë¥˜ ë°œìƒ!")
    print("="*80)
    print(f"ğŸ“Œ ì˜¤ë¥˜ íƒ€ì…: {error_module}.{error_type}")
    print(f"ğŸ“Œ ì˜¤ë¥˜ ë©”ì‹œì§€: {str(e)}")
    print("-"*80)
    
    # =========================================================================
    # 2. ì˜¤ë¥˜ íƒ€ì…ë³„ ìƒì„¸ ì§„ë‹¨
    # =========================================================================
    
    # httpx ì—°ê²° ì˜¤ë¥˜
    if httpx and isinstance(e, httpx.ConnectError):
        print("ğŸ’¡ ì§„ë‹¨: ì„œë²„ ì—°ê²° ì‹¤íŒ¨")
        print("   ì›ì¸:")
        print("   - LLM ì„œë²„ê°€ ì‹¤í–‰ë˜ì§€ ì•Šì•˜ì„ ê°€ëŠ¥ì„±")
        print("   - ë°©í™”ë²½ì´ë‚˜ ë„¤íŠ¸ì›Œí¬ ë¬¸ì œ")
        print("   - ì˜ëª»ëœ URL ì„¤ì •")
        print("\n   í•´ê²° ë°©ë²•:")
        print("   1. Ollamaë¥¼ ì‚¬ìš© ì¤‘ì´ë¼ë©´: 'ollama serve' ëª…ë ¹ìœ¼ë¡œ ì„œë²„ ì‹œì‘")
        print("   2. LM Studioë¥¼ ì‚¬ìš© ì¤‘ì´ë¼ë©´: LM Studio ì•±ì—ì„œ ì„œë²„ ì‹œì‘")
        print("   3. URL í™•ì¸: .env íŒŒì¼ì˜ OPENAI_API_BASE ì„¤ì • í™•ì¸")
    
    # httpx íƒ€ì„ì•„ì›ƒ ì˜¤ë¥˜
    elif httpx and isinstance(e, httpx.TimeoutException):
        print("ğŸ’¡ ì§„ë‹¨: ì„œë²„ ì‘ë‹µ ì‹œê°„ ì´ˆê³¼")
        print("   ì›ì¸:")
        print("   - ì„œë²„ê°€ ê³¼ë¶€í•˜ ìƒíƒœ")
        print("   - ëª¨ë¸ ë¡œë”©ì— ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¼")
        print("   - ë„¤íŠ¸ì›Œí¬ ì§€ì—°")
        print("\n   í•´ê²° ë°©ë²•:")
        print("   1. ì„œë²„ ìƒíƒœ í™•ì¸")
        print("   2. ë” ê°€ë²¼ìš´ ëª¨ë¸ë¡œ ë³€ê²½")
        print("   3. timeout ì„¤ì • ì¦ê°€")
    
    # httpx RemoteProtocolError
    elif httpx and isinstance(e, httpx.RemoteProtocolError):
        print("ğŸ’¡ ì§„ë‹¨: ì„œë²„ í”„ë¡œí† ì½œ ì˜¤ë¥˜ (ì‘ë‹µ ì¤‘ë‹¨)")
        print("   ì›ì¸:")
        print("   - ì„œë²„ê°€ ì˜ˆìƒì¹˜ ëª»í•˜ê²Œ ì—°ê²°ì„ ëŠìŒ")
        print("   - ìš”ì²­ í˜•ì‹ì´ ì„œë²„ì™€ ë§ì§€ ì•ŠìŒ")
        print("   - ì„œë²„ ë‚´ë¶€ ì˜¤ë¥˜")
        print("\n   í•´ê²° ë°©ë²•:")
        print("   1. ì„œë²„ ë¡œê·¸ í™•ì¸ (Ollama: í„°ë¯¸ë„ ì¶œë ¥, LM Studio: ì•± ë¡œê·¸)")
        print("   2. ì„œë²„ ì¬ì‹œì‘")
        print("   3. ëª¨ë¸ì´ ì •ìƒì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆëŠ”ì§€ í™•ì¸")
        print("      - Ollama: 'ollama list' ëª…ë ¹ìœ¼ë¡œ ëª¨ë¸ í™•ì¸")
        print("      - LM Studio: ë¡œë“œëœ ëª¨ë¸ í™•ì¸")
    
    # OpenAI API ìƒíƒœ ì˜¤ë¥˜
    elif openai and isinstance(e, openai.APIStatusError):
        status_code = getattr(e, 'status_code', 'Unknown')
        print(f"ğŸ’¡ ì§„ë‹¨: API ìƒíƒœ ì˜¤ë¥˜ (HTTP {status_code})")
        print("   ì›ì¸:")
        if status_code == 404:
            print("   - ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ (ëª¨ë¸ëª…ì´ ì˜ëª»ë˜ì—ˆì„ ê°€ëŠ¥ì„±)")
        elif status_code == 401:
            print("   - ì¸ì¦ ì‹¤íŒ¨ (API í‚¤ê°€ ì˜ëª»ë˜ì—ˆì„ ê°€ëŠ¥ì„±)")
        elif status_code == 500:
            print("   - ì„œë²„ ë‚´ë¶€ ì˜¤ë¥˜")
        else:
            print(f"   - HTTP {status_code} ì˜¤ë¥˜")
        print("\n   í•´ê²° ë°©ë²•:")
        print("   1. .env íŒŒì¼ì˜ OPENAI_MODEL ì„¤ì • í™•ì¸")
        print("   2. ì„œë²„ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ í™•ì¸")
    
    # OpenAI API ì—°ê²° ì˜¤ë¥˜
    elif openai and isinstance(e, openai.APIConnectionError):
        print("ğŸ’¡ ì§„ë‹¨: API ì—°ê²° ì˜¤ë¥˜")
        print("   ì›ì¸:")
        print("   - API ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŒ")
        print("   - ë„¤íŠ¸ì›Œí¬ ë¬¸ì œ")
        print("\n   í•´ê²° ë°©ë²•:")
        print("   1. ì¸í„°ë„· ì—°ê²° í™•ì¸")
        print("   2. í”„ë¡ì‹œ ì„¤ì • í™•ì¸")
        print("   3. ë°©í™”ë²½ ì„¤ì • í™•ì¸")
    
    # ê¸°íƒ€ ì˜¤ë¥˜
    else:
        print("ğŸ’¡ ì§„ë‹¨: ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜ íƒ€ì…")
        print(f"   ì˜¤ë¥˜ ê°ì²´ íƒ€ì…: {type(e)}")
        print(f"   ìƒì„¸ ë©”ì‹œì§€: {str(e)}")
    
    # =========================================================================
    # 3. í™˜ê²½ ë³€ìˆ˜ ì„¤ì • ìƒíƒœ ì¶œë ¥
    # =========================================================================
    print("\n" + "-"*80)
    print("ğŸ” í˜„ì¬ í™˜ê²½ ë³€ìˆ˜ ì„¤ì •:")
    print("-"*80)
    
    env_vars = {
        "OPENAI_API_BASE": os.getenv("OPENAI_API_BASE"),
        "OPENAI_API_KEY": "****" + (os.getenv("OPENAI_API_KEY", "")[-4:] if os.getenv("OPENAI_API_KEY") else "ì—†ìŒ"),
        "OPENAI_MODEL": os.getenv("OPENAI_MODEL"),
        "EMBEDDING_PROVIDER": os.getenv("EMBEDDING_PROVIDER"),
        "OLLAMA_EMBEDDING_MODEL": os.getenv("OLLAMA_EMBEDDING_MODEL"),
    }
    
    for key, value in env_vars.items():
        print(f"   {key}: {value or '(ì„¤ì •ë˜ì§€ ì•ŠìŒ)'}")
    
    # =========================================================================
    # 4. ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤ ì¶œë ¥
    # =========================================================================
    print("\n" + "-"*80)
    print("ğŸ“‹ ì „ì²´ ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤:")
    print("-"*80)
    traceback.print_exc()
    
    # =========================================================================
    # 5. ì—°ê²° í…ŒìŠ¤íŠ¸ ë°©ë²• ì•ˆë‚´
    # =========================================================================
    print("\n" + "-"*80)
    print("ğŸ”§ ì—°ê²° í…ŒìŠ¤íŠ¸ ë°©ë²•:")
    print("-"*80)
    api_base = os.getenv("OPENAI_API_BASE", "http://localhost:11434")
    
    if "11434" in api_base:
        print("   Ollama ì„œë²„ í…ŒìŠ¤íŠ¸:")
        print(f"   curl {api_base.replace('/v1', '')}")
        print("   (ì •ìƒì´ë©´ 'Ollama is running' ë©”ì‹œì§€ í‘œì‹œ)")
    elif "1234" in api_base:
        print("   LM Studio ì„œë²„ í…ŒìŠ¤íŠ¸:")
        print(f"   curl {api_base}/models")
        print("   (ì •ìƒì´ë©´ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ JSON ë°˜í™˜)")
    else:
        print(f"   API ì„œë²„ í…ŒìŠ¤íŠ¸:")
        print(f"   curl {api_base}")
    
    print("="*80 + "\n")
