# -*- coding: utf-8 -*-
"""
LLM íŒ©í† ë¦¬ ëª¨ë“ˆ

ì´ ëª¨ë“ˆì€ ë‹¤ì–‘í•œ LLM ì œê³µì(OpenAI, Ollama ë“±)ë¥¼ ìœ„í•œ 
íŒ©í† ë¦¬ íŒ¨í„´ì„ êµ¬í˜„í•©ë‹ˆë‹¤. ì„¤ì •ì— ë”°ë¼ ì ì ˆí•œ LLM ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

ì£¼ìš” ê¸°ëŠ¥:
    - OpenAI ChatGPT ëª¨ë¸ ìƒì„± (Local LLM í˜¸í™˜)
    - ì„ë² ë”© ëª¨ë¸ ìƒì„±
    - ì‹±ê¸€í†¤ ìºì‹± ì§€ì›
"""

import logging
import os
from typing import Optional

from dotenv import load_dotenv
from langchain_core.language_models import BaseChatModel
from langchain_core.embeddings import Embeddings

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

logger = logging.getLogger(__name__)


class LLMFactory:
    """
    LLM ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±í•˜ëŠ” íŒ©í† ë¦¬ í´ë˜ìŠ¤
    """
    
    _llm_cache: Optional[BaseChatModel] = None
    _embeddings_cache: dict[str, Embeddings] = {}
    
    @classmethod
    def create_openai_llm(
        cls,
        api_key: str,
        model: str = "gpt-4o-mini",
        temperature: float = 0.0,
        base_url: Optional[str] = None,
        **kwargs
    ) -> BaseChatModel:
        """OpenAI ChatGPT ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±"""
        
        # ìºì‹œ í™•ì¸ (kwargsê°€ ì—†ì„ ë•Œë§Œ ìºì‹œ ì‚¬ìš©)
        if not kwargs and cls._llm_cache is not None:
            return cls._llm_cache

        logger.info(f"LLM ìƒì„± ì¤‘... (ëª¨ë¸: {model}, URL: {base_url})")
        
        from langchain_openai import ChatOpenAI
        
        llm = ChatOpenAI(
            api_key=api_key or "dummy-key",
            model=model,
            temperature=temperature,
            base_url=base_url,
            **kwargs
        )
        
        # ìºì‹œ ì €ì¥ (kwargsê°€ ì—†ì„ ë•Œë§Œ)
        if not kwargs:
            cls._llm_cache = llm

        return llm
    
    @classmethod
    def create_openai_embeddings(
        cls,
        api_key: str,
        model: str = "text-embedding-3-small",
        **kwargs
    ) -> Embeddings:
        """OpenAI ì„ë² ë”© ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±"""
        
        # ìºì‹œ í‚¤ ìƒì„±
        cache_key = f"openai_{model}"
        
        if not kwargs and cache_key in cls._embeddings_cache:
            return cls._embeddings_cache[cache_key]

        logger.info(f"OpenAI ì„ë² ë”© ëª¨ë¸ ìƒì„± ì¤‘... (ëª¨ë¸: {model})")
        
        from langchain_openai import OpenAIEmbeddings
        
        embeddings = OpenAIEmbeddings(
            api_key=api_key or "dummy-key",
            model=model,
            base_url=kwargs.pop("base_url", None),
            **kwargs
        )
        
        if not kwargs:
            cls._embeddings_cache[cache_key] = embeddings

        return embeddings
    
    @classmethod
    def create_ollama_embeddings(
        cls,
        model: str = "nomic-embed-text",
        base_url: Optional[str] = None,
        **kwargs
    ) -> Embeddings:
        """Ollama ì„ë² ë”© ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        
        Args:
            model: Ollama ì„ë² ë”© ëª¨ë¸ëª… (ì˜ˆ: nomic-embed-text, mxbai-embed-large)
            base_url: Ollama ì„œë²„ URL (ê¸°ë³¸ê°’: http://localhost:11434)
            **kwargs: ì¶”ê°€ íŒŒë¼ë¯¸í„°
            
        Returns:
            Embeddings: Ollama ì„ë² ë”© ì¸ìŠ¤í„´ìŠ¤
        """
        
        # ìºì‹œ í‚¤ ìƒì„±
        cache_key = f"ollama_{model}"
        
        # ìºì‹œ í™•ì¸ (kwargsê°€ ì—†ì„ ë•Œë§Œ ìºì‹œ ì‚¬ìš©)
        if not kwargs and cache_key in cls._embeddings_cache:
            return cls._embeddings_cache[cache_key]

        logger.info(f"Ollama ì„ë² ë”© ëª¨ë¸ ìƒì„± ì¤‘... (ëª¨ë¸: {model}, URL: {base_url})")
        
        from langchain_ollama import OllamaEmbeddings
        
        # OllamaEmbeddings ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        # base_urlì´ Noneì´ë©´ OllamaEmbeddingsì˜ ê¸°ë³¸ê°’(http://localhost:11434) ì‚¬ìš©
        embeddings = OllamaEmbeddings(
            model=model,
            base_url=base_url,
            **kwargs
        )
        
        # ìºì‹œ ì €ì¥ (kwargsê°€ ì—†ì„ ë•Œë§Œ)
        if not kwargs:
            cls._embeddings_cache[cache_key] = embeddings

        return embeddings


def get_llm(**kwargs) -> BaseChatModel:
    """LLM ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜ (ì‹±ê¸€í†¤)"""
    # í™˜ê²½ë³€ìˆ˜ì—ì„œ LLM ì„¤ì • ë¡œë“œ
    api_key = os.getenv("OPENAI_API_KEY", "lm-studio")
    model = os.getenv("OPENAI_MODEL", "local-model")
    api_base = os.getenv("OPENAI_API_BASE", "http://localhost:1234/v1")
    
    return LLMFactory.create_openai_llm(
        api_key=api_key,
        model=model,
        base_url=api_base,
        **kwargs
    )


def get_embeddings(**kwargs) -> Embeddings:
    """ì„ë² ë”© ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜ (ì‹±ê¸€í†¤)
    
    í™˜ê²½ë³€ìˆ˜ EMBEDDING_PROVIDERì— ë”°ë¼ OpenAI ë˜ëŠ” Ollama ì„ë² ë”©ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    
    Args:
        **kwargs: ì„ë² ë”© ëª¨ë¸ ìƒì„± ì‹œ ì „ë‹¬í•  ì¶”ê°€ íŒŒë¼ë¯¸í„°
        
    Returns:
        Embeddings: ì„¤ì •ëœ providerì— ë§ëŠ” ì„ë² ë”© ì¸ìŠ¤í„´ìŠ¤
        
    Example:
        >>> embeddings = get_embeddings()  # EMBEDDING_PROVIDERì— ë”°ë¼ ìë™ ì„ íƒ
        >>> vectors = embeddings.embed_documents(["Hello", "World"])
    """
    # ì¸ìë¡œ ë„˜ì–´ì˜¨ providerë¥¼ ìš°ì„ í•˜ê³  ì—†ìœ¼ë©´ í™˜ê²½ë³€ìˆ˜ ì‚¬ìš©
    provider = kwargs.pop("provider", os.getenv("EMBEDDING_PROVIDER", "openai")).lower()
    
    if provider == "ollama":
        # Ollama ì„ë² ë”© ì‚¬ìš© (ë¡œì»¬ ê¸°ë³¸ ì„¤ì • ì‚¬ìš©)
        model = os.getenv("OLLAMA_EMBEDDING_MODEL", "nomic-embed-text")
        
        logger.info(f"Ollama ì„ë² ë”© provider ì‚¬ìš© (ëª¨ë¸: {model}, ë¡œì»¬ ê¸°ë³¸ ì„¤ì •)")
        return LLMFactory.create_ollama_embeddings(
            model=model,
            **kwargs
        )
    else:
        # OpenAI ì„ë² ë”© ì‚¬ìš© (ê¸°ë³¸ê°’)
        api_key = os.getenv("OPENAI_API_KEY", "lm-studio")
        model = os.getenv("OPENAI_EMBEDDING_MODEL", "local-embedding-model")
        embedding_base = os.getenv("OPENAI_EMBEDDING_API_BASE") or os.getenv("OPENAI_API_BASE", "http://localhost:1234/v1")
        
        logger.info(f"OpenAI ì„ë² ë”© provider ì‚¬ìš© (ëª¨ë¸: {model})")
        return LLMFactory.create_openai_embeddings(
            api_key=api_key,
            model=model,
            base_url=embedding_base,
            **kwargs
        )


def log_llm_error(e: Exception):
    """LLM ê´€ë ¨ ì˜¤ë¥˜ ìƒì„¸ ë¡œê¹…"""
    import openai
    import httpx
    
    error_type = type(e).__name__
    logger.error(f"âŒ LLM ì˜¤ë¥˜ ë°œìƒ! (Type: {error_type})")
    
    if isinstance(e, httpx.ConnectError):
        logger.error(f"ğŸ’¡ ì›ì¸: ì„œë²„ ì—°ê²° ì‹¤íŒ¨. Local LLMì´ ì¼œì ¸ ìˆëŠ”ì§€, URLì´ ì˜¬ë°”ë¥¸ì§€ í™•ì¸í•˜ì„¸ìš”.")
    elif isinstance(e, openai.APIStatusError):
        logger.error(f"ğŸ’¡ ì›ì¸: API ìƒíƒœ ì˜¤ë¥˜ ({e.status_code}). ëª¨ë¸ëª…ì´ë‚˜ ì„œë²„ ìƒíƒœë¥¼ í™•ì¸í•˜ì„¸ìš”.")
    else:
        logger.error(f"âš ï¸ ìƒì„¸: {str(e)}")
