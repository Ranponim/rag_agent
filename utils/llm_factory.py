# -*- coding: utf-8 -*-
"""
LLM íŒ©í† ë¦¬ ëª¨ë“ˆ

ì´ ëª¨ë“ˆì€ ë‹¤ì–‘í•œ LLM ì œê³µì(OpenAI, Ollama ë“±)ë¥¼ ìœ„í•œ 
íŒ©í† ë¦¬ íŒ¨í„´ì„ êµ¬í˜„í•©ë‹ˆë‹¤. ì„¤ì •ì— ë”°ë¼ ì ì ˆí•œ LLM ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

ì£¼ìš” ê¸°ëŠ¥:
    - OpenAI ChatGPT ëª¨ë¸ ìƒì„± (Local LLM í˜¸í™˜)
    - ì„ë² ë”© ëª¨ë¸ ìƒì„±
    - ì‹±ê¸€í†¤ ìºì‹± ì§€ì›
"""

import logging
from typing import Optional

from langchain_core.language_models import BaseChatModel
from langchain_core.embeddings import Embeddings

logger = logging.getLogger(__name__)


class LLMFactory:
    """
    LLM ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±í•˜ëŠ” íŒ©í† ë¦¬ í´ë˜ìŠ¤
    """
    
    _llm_cache: Optional[BaseChatModel] = None
    _embeddings_cache: Optional[Embeddings] = None
    
    @classmethod
    def create_openai_llm(
        cls,
        api_key: str,
        model: str = "gpt-4o-mini",
        temperature: float = 0.0,
        base_url: Optional[str] = None,
        **kwargs
    ) -> BaseChatModel:
        """OpenAI ChatGPT ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±"""
        
        # ìºì‹œ í™•ì¸ (kwargsê°€ ì—†ì„ ë•Œë§Œ ìºì‹œ ì‚¬ìš©)
        if not kwargs and cls._llm_cache is not None:
            return cls._llm_cache

        logger.info(f"LLM ìƒì„± ì¤‘... (ëª¨ë¸: {model}, URL: {base_url})")
        
        from langchain_openai import ChatOpenAI
        
        llm = ChatOpenAI(
            api_key=api_key or "dummy-key",
            model=model,
            temperature=temperature,
            base_url=base_url,
            **kwargs
        )
        
        # ìºì‹œ ì €ì¥ (kwargsê°€ ì—†ì„ ë•Œë§Œ)
        if not kwargs:
            cls._llm_cache = llm

        return llm
    
    @classmethod
    def create_openai_embeddings(
        cls,
        api_key: str,
        model: str = "text-embedding-3-small",
        **kwargs
    ) -> Embeddings:
        """OpenAI ì„ë² ë”© ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±"""
        
        if not kwargs and cls._embeddings_cache is not None:
            return cls._embeddings_cache

        logger.info(f"ì„ë² ë”© ëª¨ë¸ ìƒì„± ì¤‘... (ëª¨ë¸: {model})")
        
        from langchain_openai import OpenAIEmbeddings
        
        embeddings = OpenAIEmbeddings(
            api_key=api_key or "dummy-key",
            model=model,
            base_url=kwargs.pop("base_url", None),
            **kwargs
        )
        
        if not kwargs:
            cls._embeddings_cache = embeddings

        return embeddings


def get_llm(**kwargs) -> BaseChatModel:
    """LLM ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜ (ì‹±ê¸€í†¤)"""
    from config.settings import get_settings
    settings = get_settings()
    
    return LLMFactory.create_openai_llm(
        api_key=settings.openai_api_key,
        model=settings.openai_model,
        base_url=settings.openai_api_base,
        **kwargs
    )


def get_embeddings(**kwargs) -> Embeddings:
    """ì„ë² ë”© ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜ (ì‹±ê¸€í†¤)"""
    from config.settings import get_settings
    settings = get_settings()
    
    embedding_base_url = settings.openai_embedding_api_base or settings.openai_api_base
    
    return LLMFactory.create_openai_embeddings(
        api_key=settings.openai_api_key,
        model=settings.openai_embedding_model,
        base_url=embedding_base_url,
        **kwargs
    )


def log_llm_error(e: Exception):
    """LLM ê´€ë ¨ ì˜¤ë¥˜ ìƒì„¸ ë¡œê¹…"""
    import openai
    import httpx
    
    error_type = type(e).__name__
    logger.error(f"âŒ LLM ì˜¤ë¥˜ ë°œìƒ! (Type: {error_type})")
    
    if isinstance(e, httpx.ConnectError):
        logger.error(f"ğŸ’¡ ì›ì¸: ì„œë²„ ì—°ê²° ì‹¤íŒ¨. Local LLMì´ ì¼œì ¸ ìˆëŠ”ì§€, URLì´ ì˜¬ë°”ë¥¸ì§€ í™•ì¸í•˜ì„¸ìš”.")
    elif isinstance(e, openai.APIStatusError):
        logger.error(f"ğŸ’¡ ì›ì¸: API ìƒíƒœ ì˜¤ë¥˜ ({e.status_code}). ëª¨ë¸ëª…ì´ë‚˜ ì„œë²„ ìƒíƒœë¥¼ í™•ì¸í•˜ì„¸ìš”.")
    else:
        logger.error(f"âš ï¸ ìƒì„¸: {str(e)}")
